name: ü§ñ Gemini AI Production PDR Processing

on:
  workflow_dispatch:
    inputs:
      date:
        description: 'Target date (YYYY-MM-DD)'
        required: true
        default: '2025-07-14'
      instructions:
        description: 'Additional processing instructions (optional)'
        required: false
        default: ''

jobs:
  gemini-pdr-processing:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
      pull-requests: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Check Bridge Health & Extract WhatsApp Messages
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: extract
      run: |
        CODESPACE_NAME="cuddly-guacamole-496vp6p46wg39r"
        TARGET_DATE="${{ github.event.inputs.date }}"
        
        echo "üîç Checking WhatsApp bridge health and extracting messages for $TARGET_DATE..."
        
        # Check if bridge service is running and restart if needed
        echo "üîß Checking bridge service status..."
        BRIDGE_STATUS=$(gh codespace ssh -c $CODESPACE_NAME -- \
          "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh status" 2>/dev/null || echo "Bridge not running")
        
        if [[ "$BRIDGE_STATUS" == *"not running"* ]]; then
          echo "üö® Bridge not running - starting service..."
          gh codespace ssh -c $CODESPACE_NAME -- \
            "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh start"
          sleep 10  # Give bridge time to start
          echo "‚úÖ Bridge service started"
        else
          echo "‚úÖ Bridge service is running"
        fi
        
        # Extract messages from Codespace SQLite database
        echo "üìã Extracting messages with production keywords..."
        gh codespace ssh -c $CODESPACE_NAME -- \
          "sqlite3 /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db \"SELECT timestamp, chat_jid, sender, content FROM messages WHERE timestamp BETWEEN '${TARGET_DATE} 00:00:00+00:00' AND '${TARGET_DATE} 23:59:59+00:00' AND chat_jid = '27834418149-1537194373@g.us' AND (content LIKE '%ROM%' OR content LIKE '%Safety%' OR content LIKE '%Gloria Report%' OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%' OR content LIKE '%Decline%' OR content LIKE '%Product%' OR content LIKE '%Loads%') ORDER BY timestamp;\"" > extracted_messages.txt
        
        # Count messages and validate extraction
        MESSAGE_COUNT=$(wc -l < extracted_messages.txt)
        echo "üìä Found $MESSAGE_COUNT WhatsApp messages for $TARGET_DATE"
        
        # Output variables for next steps
        echo "message_count=$MESSAGE_COUNT" >> $GITHUB_OUTPUT
        echo "target_date=$TARGET_DATE" >> $GITHUB_OUTPUT
        
        # Save extracted data for Gemini processing
        if [ "$MESSAGE_COUNT" -gt 0 ]; then
          cp extracted_messages.txt whatsapp_data_for_gemini.txt
          echo "data_file_created=true" >> $GITHUB_OUTPUT
        else
          echo "data_file_created=false" >> $GITHUB_OUTPUT
        fi
        
        # Preview extracted data (first 3 lines)
        echo "üìã Message preview:"
        head -3 extracted_messages.txt || echo "No data to preview"
    
    - name: Handle No Data Found
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: steps.extract.outputs.message_count == '0'
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        echo "üõë STOPPING PROCESSING - No data available for $TARGET_DATE"
        echo "Possible reasons:"
        echo "  - Date may be a weekend/holiday with no reports"
        echo "  - Bridge may have been offline during report time"
        echo "  - Reports may use different keywords not in search filter"
        echo "  - Date format may be incorrect (use YYYY-MM-DD)"
        
        # Create a no-data notification issue
        gh issue create \
          --title "‚ùå No Data Found (Gemini Production): $TARGET_DATE" \
          --body "**No production data available for $TARGET_DATE**

        üìä **Search Results**: 0 messages found
        üîç **Search Criteria**: ROM, Production, Gloria, Nchwaning, S&W keywords
        ‚è∞ **Time Range**: Full day (00:00-23:59 UTC)
        ü§ñ **Processing**: Gemini AI Production workflow
        
        **Possible Reasons**:
        - Weekend/holiday - no production reports sent
        - WhatsApp bridge offline during report time (4:00-6:00 AM SAST)
        - Engineers used different messaging keywords
        - Date format incorrect or date does not exist
        
        **Bridge Status**: Checked and restarted if needed
        **Recommendation**: Verify date and retry, or process manually if reports exist" \
          --label "no-data" \
          --label "pdr-gemini"
        
        echo "‚úÖ Created no-data notification issue"
        
    - name: Process with Gemini AI (Production Logic)
      uses: 'google-github-actions/run-gemini-cli@v0'
      id: gemini_process
      if: steps.extract.outputs.message_count > 0
      env:
        TARGET_DATE: ${{ github.event.inputs.date }}
        MESSAGE_COUNT: ${{ steps.extract.outputs.message_count }}
      with:
        gemini_model: 'gemini-2.5-flash'
        gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
        settings: |-
          {
            "maxSessionTurns": 20,
            "autoAccept": ["list_directory", "read_file", "write_file", "glob"],
            "telemetry": {
              "enabled": false,
              "target": "gcp"
            }
          }
        prompt: |-
          ## Role
          
          You are a specialized AI assistant for processing daily production reports for Assmang's Black Rock mining operations. You have full access to all available tools to interact with the repository, create files, and manage the workflow.
          
          ## Context
          
          - **Repository**: `${{ github.repository }}`
          - **Processing Date**: `${{ github.event.inputs.date }}`
          - **Messages Found**: `${{ steps.extract.outputs.message_count }}`
          - **Data Source**: WhatsApp production reports from mine engineers
          - **Output Required**: JSON and Markdown files for each mine site
          
          ## Task: Daily Production Report Processing
          
          You need to process WhatsApp production data and create comprehensive daily reports for each detected mine site.
          
          **SITES TO PROCESS**:
          - Gloria Mine (Engineer: Sipho Dubazane)
          - Nchwaning 2 (Engineer: Sikelela Nzuza) 
          - Nchwaning 3 (Engineer: Sello Sease)
          - Shafts & Winders (Engineer: Xavier Peterson)
          
          ## How to Process Production Reports
          
          ### Step 1: Create and Maintain a Plan
          **Show and maintain a plan as a checklist**:
          - At the very beginning, outline the steps needed to process the production data
          - Create a plan comment in the workflow logs or as output
          - Example plan:
            ```
            ### Production Report Processing Plan
            - [ ] Read and analyze WhatsApp data source
            - [ ] Identify mine sites present in data
            - [ ] Extract production metrics for each site
            - [ ] Create JSON files with structured data
            - [ ] Create Markdown reports for readability
            - [ ] Validate all data points against source
            - [ ] Confirm file creation and structure
            ```
          
          ### Step 2: Data Analysis and Extraction
          1. **Read the source data**: Use `cat whatsapp_data_for_gemini.txt` or appropriate tools to examine the WhatsApp production data
          2. **Identify mine sites**: Look for mentions of Gloria, Nchwaning 2, Nchwaning 3, and Shafts & Winders
          3. **Extract key metrics**: 
             - Safety status and incidents
             - Production figures (ROM, Product, Decline, Loads)
             - Equipment availability (DT, FL percentages)
             - Silo/Dam levels where applicable
          
          ### Step 3: File Creation
          **Create files using available tools**:
          
          **Directory Structure**: `daily_production/data/YYYY-MM/DD/`
          
          **For each site found in data:**
          
          1. **JSON File** (`YYYY-MM-DD_[site].json`):
             ```json
             {
               "report_metadata": {
                 "date": "${{ github.event.inputs.date }}",
                 "site": "[Site Name]",
                 "engineer": "[Engineer Name]",
                 "processing_method": "gemini_ai_automated"
               },
               "safety": {
                 "status": "[extracted from source]",
                 "incidents": [number]
               },
               "production": {
                 "rom": {"actual": [number], "unit": "tonnes"},
                 "decline": {"actual": [number], "unit": "metres"},
                 "product": {"actual": [number], "unit": "tonnes"}
               },
               "equipment_availability": {
                 "tmm": {
                   "DT": [percentage],
                   "FL": [percentage]
                 }
               },
               "source_validation": {
                 "rom_actual": {
                   "value": [number],
                   "source_quote": "[exact text from WhatsApp]",
                   "confidence": "HIGH|MEDIUM|LOW"
                 }
               }
             }
             ```
          
          2. **Markdown File** (`YYYY-MM-DD ‚Äì [Site Name] Daily Report.md`):
             - Professional summary of the day's operations
             - Key production metrics highlighted
             - Safety status and any incidents
             - Equipment performance summary
             - Issues or concerns noted
          
          ### Step 4: Data Validation Requirements
          
          **CRITICAL DATA INTEGRITY RULES**:
          - **NEVER INVENT DATA**: Extract only actual values from the WhatsApp source
          - **Source Traceability**: Every data point must reference the exact source text
          - **Use null for missing data**: If a metric isn't in the source, use `null`
          - **Quote Accuracy**: Include exact quotes from WhatsApp messages
          - **Confidence Levels**: Mark data confidence as HIGH/MEDIUM/LOW based on clarity
          
          ### Step 5: File Organization and Verification
          
          **CRITICAL FILE PATH REQUIREMENTS**:
          - **Use ABSOLUTE paths only**: All file paths must start with `/github/workspace/` or be relative to repository root
          - **Example JSON path**: `/github/workspace/daily_production/data/2025-07/16/2025-07-16_gloria.json`
          - **Example MD path**: `/github/workspace/daily_production/data/2025-07/16/2025-07-16 ‚Äì Gloria Daily Report.md`
          
          **MANDATORY CREATION STEPS**:
          1. **Create directory structure**: Use `mkdir -p` to ensure `daily_production/data/YYYY-MM/DD/` exists
          2. **Create JSON file FIRST** for each site with absolute path
          3. **Create Markdown file SECOND** for each site with absolute path
          4. **Verify both files exist** using `ls` command
          5. **Keep processing simple**: Create files for available sites only, skip missing data
          
          **RATE LIMIT HANDLING**:
          - Work efficiently to minimize API calls
          - Focus on data extraction first, then file creation
          - If you hit rate limits, create files with the data already extracted
          
          ## Guidelines
          
          - **Be systematic and thorough**: Follow the checklist plan step by step
          - **Maintain data integrity**: Every piece of data must be traceable to the source
          - **Use all available tools**: You have access to file operations, directory management, and analysis tools
          - **Create professional output**: Both JSON and Markdown should be well-formatted and complete
          - **Handle errors gracefully**: If data is missing or unclear, document this in the output
          - **Follow project conventions**: Use the established file naming and directory structure
          
          ## Success Criteria
          
          - [ ] WhatsApp data successfully analyzed
          - [ ] All mine sites identified and processed
          - [ ] JSON files created with complete schema
          - [ ] Markdown reports generated for readability
          - [ ] All data points validated against source
          - [ ] Files properly organized in directory structure
          - [ ] Processing plan completed and verified
          
          **Begin processing the production data for ${{ github.event.inputs.date }} now.**
          
          ## Available Tools
          
          You have access to all available MCP (Model Context Protocol) tools including:
          - **File Operations**: `write_file`, `read_file`, `list_files` for creating and managing files
          - **Directory Management**: Tools to create directory structures
          - **Data Analysis**: Tools to read and process the WhatsApp source data
          - **Git Operations**: If needed for repository management
          
          Use these tools systematically to:
          1. Read the WhatsApp data source: `whatsapp_data_for_gemini.txt`
          2. Create the required directory structure: `daily_production/data/YYYY-MM/DD/`
          3. Generate JSON files with structured production data
          4. Create Markdown reports for human readability
          5. Validate all file creation was successful
          
          ## Important Notes
          
          - **File Paths**: Use absolute paths relative to repository root
          - **Data Integrity**: Never fabricate data - extract only from WhatsApp source
          - **Validation**: Include source quotes for all extracted metrics
          - **Quality**: Follow professional standards for both JSON schema and Markdown formatting
          
          **CRITICAL**: This is production data processing for actual mining operations. Accuracy is paramount.
    
    - name: Verify File Creation
      id: verify_files
      if: steps.gemini_process.conclusion == 'success'
      run: |
        TARGET_DATE="${{ github.event.inputs.date }}"
        YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
        DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
        TARGET_DIR="daily_production/data/${YEAR_MONTH}/${DAY}"
        
        echo "üîç Verifying files were created by Gemini production processing in: $TARGET_DIR"
        
        # Count actual files created
        JSON_FILES=$(find "$TARGET_DIR" -name "*.json" 2>/dev/null | wc -l || echo "0")
        MD_FILES=$(find "$TARGET_DIR" -name "*.md" 2>/dev/null | wc -l || echo "0")
        TOTAL_FILES=$((JSON_FILES + MD_FILES))
        
        echo "üìä Verification: $JSON_FILES JSON, $MD_FILES Markdown = $TOTAL_FILES total files"
        
        # List created files
        if [ -d "$TARGET_DIR" ]; then
          echo "üìÇ Files in $TARGET_DIR:"
          ls -la "$TARGET_DIR" || echo "Directory exists but is empty"
        else
          echo "‚ö†Ô∏è Target directory does not exist"
        fi
        
        # Validation check
        if [ "$TOTAL_FILES" -gt 0 ]; then
          echo "‚úÖ Files were created by Gemini ($TOTAL_FILES total)"
        else
          echo "‚ùå No files were created by Gemini"
        fi
        
        echo "json_files_count=$JSON_FILES" >> $GITHUB_OUTPUT
        echo "md_files_count=$MD_FILES" >> $GITHUB_OUTPUT
        echo "total_files=$TOTAL_FILES" >> $GITHUB_OUTPUT
        echo "target_dir=$TARGET_DIR" >> $GITHUB_OUTPUT
  
    - name: Commit and Push Generated Reports
      if: always()
      run: |
          TARGET_DATE="${{ github.event.inputs.date }}"
          TOTAL_FILES="${{ steps.verify_files.outputs.total_files }}"
          JSON_FILES="${{ steps.verify_files.outputs.json_files_count }}"
          MD_FILES="${{ steps.verify_files.outputs.md_files_count }}"
          
          echo "üîç DEBUG: total_files='$TOTAL_FILES', json_files='$JSON_FILES', md_files='$MD_FILES'"
          
          # Check if any files were actually created by looking in the target directory
          YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
          DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
          TARGET_DIR="daily_production/data/${YEAR_MONTH}/${DAY}"
          
          echo "üîç Checking for files in: $TARGET_DIR"
          
          if [ ! -d "$TARGET_DIR" ]; then
            echo "üìÇ Directory $TARGET_DIR does not exist - no files to commit"
            exit 0
          fi
          
          # Count actual files
          ACTUAL_FILES=$(find "$TARGET_DIR" -type f | wc -l)
          echo "üìä Found $ACTUAL_FILES files to commit"
          
          if [ "$ACTUAL_FILES" -eq 0 ]; then
            echo "‚ö†Ô∏è No files found - nothing to commit"
            exit 0
          fi
          
          echo "üìù Committing $ACTUAL_FILES generated reports for $TARGET_DATE..."
          
          # Configure git
          git config --local user.email "actions@github.com"
          git config --local user.name "GitHub Actions (Gemini AI)"
          
          # Add generated files
          git add daily_production/data/
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "‚ö†Ô∏è No changes to commit (files may already be committed)"
            exit 0
          fi
          
          # Count JSON and Markdown files separately for better reporting
          JSON_COUNT=$(find "$TARGET_DIR" -name "*.json" | wc -l)
          MD_COUNT=$(find "$TARGET_DIR" -name "*.md" | wc -l)
          
          # Create commit with detailed message
          git commit -m "$(cat <<EOF
          feat: Autonomous daily production reports - $TARGET_DATE
          
          üìä **Generated Reports**: $JSON_COUNT JSON, $MD_COUNT Markdown = $ACTUAL_FILES total files
          ü§ñ **Processing Method**: Gemini AI autonomous workflow
          üìÖ **Report Date**: $TARGET_DATE
          üîç **Data Source**: WhatsApp production messages from mine engineers
          
          **Sites Processed**:
          $(if [ "$JSON_COUNT" -gt 0 ]; then find "$TARGET_DIR" -name "*.json" -exec basename {} \; | sed 's/.*_\([^_]*\)\.json$/- \1/' | sort; fi)
          
          **Workflow Features**:
          - Automatic WhatsApp bridge health checking
          - Real-time data extraction from Codespace SQLite database
          - AI-powered data structuring with source validation
          - Professional JSON schema with comprehensive metadata
          - Human-readable Markdown reports for operational review
          
          ü§ñ Generated with [Claude Code](https://claude.ai/code)
          
          Co-Authored-By: Claude <noreply@anthropic.com>
          EOF
          )"
          
          # Push changes
          git push origin master
          
          echo "‚úÖ Successfully committed and pushed $TOTAL_FILES reports for $TARGET_DATE"