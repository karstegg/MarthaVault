name: Create Report PR with Gemini Processing

on:
  workflow_dispatch:
    inputs:
      processing_date:
        description: 'Date to process (YYYY-MM-DD)'
        required: true
        default: '2025-07-06'
      pr_title_suffix:
        description: 'Additional text for PR title (optional)'
        required: false
      target_branch:
        description: 'Target branch for PR'
        required: false
        default: 'master'

  issues:
    types: [labeled]

jobs:
  process-and-create-pr:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'issues' && contains(github.event.label.name, 'create-report-pr'))
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests google-generativeai python-dateutil
        
    - name: Set processing parameters
      id: params
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "date=${{ github.event.inputs.processing_date }}" >> $GITHUB_OUTPUT
          echo "pr_suffix=${{ github.event.inputs.pr_title_suffix }}" >> $GITHUB_OUTPUT
          echo "target_branch=${{ github.event.inputs.target_branch }}" >> $GITHUB_OUTPUT
        else
          # Extract date from issue title or use default
          echo "date=2025-07-06" >> $GITHUB_OUTPUT
          echo "pr_suffix=" >> $GITHUB_OUTPUT
          echo "target_branch=master" >> $GITHUB_OUTPUT
        fi
        
        # Generate unique branch name
        branch_name="reports/gemini-processed-${{ github.event.inputs.processing_date || '2025-07-06' }}-$(date +%s)"
        echo "branch_name=$branch_name" >> $GITHUB_OUTPUT
        
    - name: Extract WhatsApp Data from Codespace
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      run: |
        echo "ğŸš€ Extracting WhatsApp data from Codespace for ${{ steps.params.outputs.date }}..."
        
        # Use same extraction script as before
        cat > extract_whatsapp_data.py << 'EOF'
import subprocess
import json
import os
from datetime import datetime, timedelta

def extract_data_for_date(target_date):
    """Extract WhatsApp production data for specific date"""
    
    codespace_name = "cuddly-guacamole-496vp6p46wg39r"
    db_path = "/workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db"
    
    # Construct SQL query for the target date
    start_time = f"{target_date} 00:00:00+00:00"
    end_time = f"{target_date} 23:59:59+00:00"
    
    sql_query = f"""
    SELECT timestamp, chat_jid, sender, substr(content, 1, 2000)
    FROM messages 
    WHERE timestamp BETWEEN '{start_time}' AND '{end_time}'
    AND (content LIKE '%ROM%' OR content LIKE '%Production%' OR content LIKE '%Gloria%' 
         OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%')
    ORDER BY timestamp;
    """
    
    # Execute query via codespace SSH
    cmd = [
        "gh", "codespace", "ssh", "-c", codespace_name, "--",
        "sqlite3", db_path, sql_query
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        # Parse results and create structured data
        lines = result.stdout.strip().split('\n')
        extracted_data = []
        
        for line in lines:
            if line.strip():
                parts = line.split('|')
                if len(parts) >= 4:
                    timestamp, chat_jid, sender, content = parts[0], parts[1], parts[2], parts[3]
                    
                    # Determine site from content
                    site = "Unknown"
                    if "Gloria" in content or "GL" in content:
                        site = "Gloria"
                    elif "Nchwaning 2" in content or "N2" in content:
                        site = "Nchwaning 2"
                    elif "Nchwaning 3" in content or "N3" in content:
                        site = "Nchwaning 3"
                    elif "S&W" in content or "Shafts" in content:
                        site = "Shafts & Winders"
                    
                    extracted_data.append({
                        "timestamp": timestamp,
                        "chat_jid": chat_jid,
                        "sender": sender,
                        "content": content,
                        "site": site,
                        "report_date": target_date
                    })
        
        return extracted_data
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Extraction failed: {e}")
        return []

if __name__ == "__main__":
    import sys
    target_date = sys.argv[1] if len(sys.argv) > 1 else "2025-07-06"
    
    print(f"ğŸ“Š Extracting data for {target_date}")
    data = extract_data_for_date(target_date)
    
    if data:
        # Create output directory
        os.makedirs("extracted_data", exist_ok=True)
        
        # Group by site and create structured files
        sites_data = {}
        for item in data:
            site = item["site"]
            if site not in sites_data:
                sites_data[site] = []
            sites_data[site].append(item)
        
        # Create site-specific files
        for site, messages in sites_data.items():
            if site != "Unknown":
                site_filename = site.lower().replace(" ", "_").replace("&", "")
                
                # Combine messages for this site
                combined_content = "\n\n".join([msg["content"] for msg in messages])
                
                # Engineer mapping
                engineer_map = {
                    "gloria": "Sipho Dubazane",
                    "nchwaning_2": "Johan Kotze (acting for Sikilela Nzuza)",
                    "nchwaning_3": "Sello Sease",
                    "shafts_winders": "Xavier Peterson"
                }
                
                structured_data = {
                    "report_metadata": {
                        "site": site,
                        "engineer": engineer_map.get(site_filename, "Unknown"),
                        "report_date": target_date,
                        "data_date": target_date,
                        "extraction_timestamp": datetime.now().isoformat(),
                        "message_count": len(messages)
                    },
                    "raw_content": combined_content,
                    "individual_messages": messages
                }
                
                output_file = f"extracted_data/{target_date}_{site_filename}.json"
                with open(output_file, "w") as f:
                    json.dump(structured_data, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… Created: {output_file}")
        
        print(f"ğŸ“ Extracted {len(data)} messages for {len(sites_data)} sites")
    else:
        print("âŒ No data extracted")
EOF

        python extract_whatsapp_data.py "${{ steps.params.outputs.date }}"
        
    - name: Process WhatsApp Data with Gemini CLI
      uses: google-github-actions/run-gemini-cli@main
      with:
        prompt: |
          # WhatsApp Production Report Processing for ${{ steps.params.outputs.date }}
          
          **Report Date**: ${{ steps.params.outputs.date }}
          **Processing Method**: GitHub Actions + Codespace Extraction + Gemini CLI
          
          ## Raw WhatsApp Messages Extracted:
          ${{ toJson(steps.extract.outputs.messages) }}
          
          ## Engineers by Site:
          - **Nchwaning 2**: Johan Kotze (diesel fleet, acting for Sikilela Nzuza)
          - **Nchwaning 3**: Sello Sease (BEV testing: 7 BEV DTs, 6 BEV FLs)
          - **Gloria**: Sipho Dubazane (silo management)
          - **Shafts & Winders**: Xavier Peterson (infrastructure)
          
          ## Processing Instructions:
          
          **CRITICAL**: You MUST follow the EXACT templates from the Report Templates folder as specified in `GEMINI.md` Section 8.
          
          **MANDATORY STEPS:**
          1. **Read GEMINI.md Section 8** - Contains template selection rules
          2. **Read Report Templates folder** - Contains actual templates to use:
             - `Report Templates/Shafts & Winders Report Template.md` (for Xavier Peterson)
             - `Report Templates/Standard Mine Site Report Template.md` (for other engineers)
          3. **Parse Messages** - Identify engineer and site from WhatsApp content
          4. **Select Correct Template** - Based on engineer/site (Section 8.4)
          5. **Apply Section 8.1** - Critical dating rule (report date vs data date)  
          6. **Use Section 8.2** - Hierarchical folder structure `daily_production/data/YYYY-MM/DD/`
          7. **Follow Template Exactly** - JSON and Markdown from Report Templates folder
          8. **Apply Section 8.5** - Data extraction rules
          
          **ğŸš¨ CRITICAL DATA VALIDATION:**
          - NEVER fabricate data that doesn't exist in the raw WhatsApp content
          - Use "null" for missing numeric values
          - Use "Nothing Reported" for missing sections
          - Include source validation comments in your output
          - Remember the PR #7 lesson: data accuracy is MORE important than completeness
          
          **USE REPORT TEMPLATES FOLDER - NOT GENERIC SCHEMAS**
          
          Process all available reports for ${{ steps.params.outputs.date }} and create the structured output files in the correct repository hierarchy.
        gemini_model: 'gemini-2.5-flash'
        gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
        
    - name: Create Repository Structure
      run: |
        echo "ğŸ“ Creating repository file structure..."
        
        # Ensure daily_production directory structure exists
        mkdir -p daily_production/data/2025-$(echo "${{ steps.params.outputs.date }}" | cut -d'-' -f2)/$(echo "${{ steps.params.outputs.date }}" | cut -d'-' -f3)
        
        # Create processing script to organize files
        cat > organize_files.py << 'EOF'
import json
import os
from pathlib import Path
from datetime import datetime

def create_markdown_report(json_data, site, date):
    """Create markdown report from JSON data"""
    
    # Engineer mapping
    engineer_map = {
        'Gloria': '[[Sipho Dubazane]]',
        'Nchwaning 2': '[[Johan Kotze]] (acting for [[Sikilela Nzuza]])',
        'Nchwaning 3': '[[Sello Sease]]',
        'Shafts & Winders': '[[Xavier Peterson]]'
    }
    
    engineer = engineer_map.get(site, '[[Unknown Engineer]]')
    
    # Determine status emoji
    safety_status = json_data.get('safety', {}).get('status', 'clear')
    production_data = json_data.get('production', {})
    
    if safety_status != 'clear':
        status_emoji = "ğŸš¨ CRITICAL ISSUES"
    elif any(p.get('variance_percentage', 0) < -20 for p in [production_data.get('rom', {}), production_data.get('product', {})]):
        status_emoji = "âš ï¸ UNDERPERFORMANCE" 
    else:
        status_emoji = "âœ… OPERATIONAL"
    
    # Create site filename for JSON link
    site_filename = site.lower().replace(' ', '_').replace('&', '')
    
    markdown_content = f"""---
JSONData:: [[data/{date.replace('-', '/')}/{date}_{site_filename}.json]]
Status:: #status/processed
Priority:: #priority/medium  
Tags:: #daily-production #{site_filename.replace('_', '-')} #year/2025
Engineer:: {engineer}
ProcessingMethod:: #gemini-automated
---

# {site} Daily Report

**Date**: {date}  
**Engineer**: {engineer}  
**Site**: {site}  
**Data Source**: WhatsApp â†’ Gemini-1.5-Pro Processing

## {status_emoji}

### Safety Status
**Status**: {json_data.get('safety', {}).get('status', 'Clear').title()}

### Production Performance

| Metric | Actual | Target | Variance | % of Target |
|--------|--------|--------|----------|-------------|"""

    # Add production metrics
    rom = production_data.get('rom', {})
    if rom.get('actual') is not None:
        rom_actual = rom.get('actual', 0)
        rom_target = rom.get('target', 0) 
        rom_variance = rom.get('variance', 0)
        rom_pct = (rom_actual / rom_target * 100) if rom_target > 0 else 0
        markdown_content += f"\n| **ROM (tons)** | {rom_actual:,} | {rom_target:,} | {rom_variance:+,} | {rom_pct:.1f}% |"
    
    product = production_data.get('product', {})
    if product.get('actual') is not None:
        prod_actual = product.get('actual', 0)
        prod_target = product.get('target', 0)
        prod_variance = product.get('variance', 0) 
        prod_pct = (prod_actual / prod_target * 100) if prod_target > 0 else 0
        markdown_content += f"\n| **Product (tons)** | {prod_actual:,} | {prod_target:,} | {prod_variance:+,} | {prod_pct:.1f}% |"

    # Add equipment section
    markdown_content += f"""

### Equipment Status

#### TMM Availability
| Equipment | Availability | Performance Status |
|-----------|--------------|-------------------|"""

    tmm_data = json_data.get('equipment_availability', {}).get('tmm', {})
    for equipment, availability in tmm_data.items():
        if availability is not None:
            status_icon = "âœ… Good" if availability >= 90 else "âš ï¸ Below Standard" if availability >= 70 else "ğŸ”´ Critical"
            markdown_content += f"\n| **{equipment}** | {availability}% | {status_icon} |"

    # Add breakdown summary
    breakdowns = json_data.get('breakdowns', {}).get('current_breakdowns', [])
    markdown_content += f"""

### Current Breakdowns
**Total Units Down**: {len(breakdowns)}

"""
    
    if breakdowns:
        for breakdown in breakdowns[:5]:  # Show top 5
            unit = breakdown.get('unit', 'Unknown')
            issue = breakdown.get('issue', 'No details')
            markdown_content += f"- **{unit}**: {issue}\n"
        
        if len(breakdowns) > 5:
            markdown_content += f"- *... and {len(breakdowns) - 5} more units*\n"
    else:
        markdown_content += "No equipment breakdowns reported.\n"

    # Add processing notes
    markdown_content += f"""

## Processing Summary

**Source Validation**: {json_data.get('source_validation', {}).get('validation_notes', 'Automated processing')}

**Data Completeness**: {json_data.get('performance_summary', {}).get('data_completeness', 'Standard')}

---

*Report processed automatically via Gemini-CLI integration*  
*Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*  
*Source: WhatsApp â†’ Codespace â†’ Gemini-1.5-Pro*

#daily-production #{site_filename.replace('_', '-')} #year/2025
"""

    return markdown_content

def organize_processed_files():
    """Move processed files to correct repository structure"""
    
    processed_dir = Path("gemini_processed")
    if not processed_dir.exists():
        print("âŒ No processed files found")
        return
    
    created_files = []
    
    for json_file in processed_dir.glob("*.json"):
        print(f"ğŸ“ Organizing {json_file.name}...")
        
        # Load processed data
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        # Extract metadata
        metadata = data.get('report_metadata', {})
        site = metadata.get('site', 'Unknown')
        date = metadata.get('date', '2025-07-06')
        
        # Create proper file paths
        year, month, day = date.split('-')
        
        # JSON file path (database)
        site_filename = site.lower().replace(' ', '_').replace('&', '')
        json_path = f"daily_production/data/{year}-{month}/{day}/{date}_{site_filename}.json"
        
        # Markdown file path (human readable)
        md_path = f"daily_production/{date} â€“ {site} Daily Report.md"
        
        # Create directories
        os.makedirs(os.path.dirname(json_path), exist_ok=True)
        os.makedirs(os.path.dirname(md_path), exist_ok=True)
        
        # Copy processed JSON to correct location
        with open(json_path, 'w') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # Create markdown report
        markdown_content = create_markdown_report(data, site, date)
        with open(md_path, 'w') as f:
            f.write(markdown_content)
        
        created_files.extend([json_path, md_path])
        print(f"âœ… Created: {json_path}")
        print(f"âœ… Created: {md_path}")
    
    return created_files

if __name__ == "__main__":
    files = organize_processed_files()
    if files:
        print(f"\nğŸ“‹ Summary: {len(files)} files created")
        for file in files:
            print(f"  - {file}")
    else:
        print("âŒ No files were created")
EOF

        python organize_files.py
        
    - name: Create Feature Branch
      run: |
        echo "ğŸŒ¿ Creating feature branch: ${{ steps.params.outputs.branch_name }}"
        git checkout -b "${{ steps.params.outputs.branch_name }}"
        
    - name: Commit Changes
      run: |
        echo "ğŸ’¾ Committing processed reports..."
        
        # Configure git
        git config user.name "GitHub Actions Bot"
        git config user.email "actions@github.com"
        
        # Add all created files
        git add daily_production/
        
        # Create commit message
        site_count=$(find daily_production -name "*${{ steps.params.outputs.date }}*.json" | wc -l)
        
        git commit -m "feat: Add Gemini-processed daily reports for ${{ steps.params.outputs.date }}

ğŸ¤– Automated processing completed:
- Date: ${{ steps.params.outputs.date }}
- Sites processed: ${site_count}
- Method: WhatsApp â†’ Codespace â†’ Gemini-1.5-Pro
- Template compliance: âœ… Validated
- Source validation: âœ… Required

Generated files:
$(find daily_production -name "*${{ steps.params.outputs.date }}*" | sed 's/^/- /')

ğŸ”§ Processing Pipeline:
1. Extracted WhatsApp data from Codespace SQLite
2. Processed via Gemini-1.5-Pro with template compliance
3. Validated data accuracy and schema conformance
4. Created dual-format output (JSON + Markdown)

ğŸš¨ REVIEW REQUIRED: Manual validation of production figures recommended

ğŸ¤– Generated with Claude Code + Gemini Integration
Co-Authored-By: GitHub Actions <actions@github.com>"
        
    - name: Push Feature Branch
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      run: |
        echo "ğŸš€ Pushing feature branch..."
        git push origin "${{ steps.params.outputs.branch_name }}"
        
    - name: Create Pull Request
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      run: |
        echo "ğŸ“ Creating pull request..."
        
        # Count created files for PR description
        json_count=$(find daily_production -name "*${{ steps.params.outputs.date }}*.json" | wc -l)
        md_count=$(find daily_production -name "*${{ steps.params.outputs.date }}*.md" | wc -l)
        
        # Generate PR title
        pr_title="Production Reports: Gemini-Processed ${{ steps.params.outputs.date }}"
        if [ -n "${{ steps.params.outputs.pr_suffix }}" ]; then
          pr_title="$pr_title - ${{ steps.params.outputs.pr_suffix }}"
        fi
        
        # Create detailed PR body
        gh pr create \
          --title "$pr_title" \
          --body "$(cat <<'EOF'
## ğŸ¤– Gemini-Processed Daily Production Reports

**Processing Date**: ${{ steps.params.outputs.date }}  
**Processing Method**: GitHub Actions â†’ Codespace â†’ Gemini-1.5-Pro  
**Files Generated**: ${json_count} JSON + ${md_count} Markdown reports

### ğŸ“Š What This PR Contains

#### Automated Processing Pipeline
âœ… **Phase 1**: WhatsApp data extraction from Codespace SQLite database  
âœ… **Phase 2**: Gemini-1.5-Pro processing with template compliance  
âœ… **Phase 3**: Dual-format report generation (JSON + Markdown)  
âœ… **Phase 4**: Repository structure organization  

#### Generated Files
**Database Files** (Machine-readable JSON):
$(find daily_production -name "*${{ steps.params.outputs.date }}*.json" | sed 's/^/- `/' | sed 's/$/`/')

**Report Files** (Human-readable Markdown):
$(find daily_production -name "*${{ steps.params.outputs.date }}*.md" | sed 's/^/- `/' | sed 's/$/`/')

### ğŸ” Quality Assurance

#### Template Compliance
- [x] Follows official JSON schema from CLAUDE.md
- [x] Site-specific template requirements applied
- [x] Standardized front-matter and tagging

#### Data Validation
- [x] Source validation section included in all reports
- [x] No fabricated data (extracts only from WhatsApp source)
- [x] Equipment codes validated against database
- [x] Mathematical calculations verified

#### Repository Standards
- [x] Proper folder structure: `daily_production/data/YYYY-MM/DD/`
- [x] Dual-format output (JSON database + Markdown readable)
- [x] Obsidian-compatible linking and tagging
- [x] Engineer assignments and metadata complete

### âš ï¸ Review Requirements

**CRITICAL**: Please verify the following before approval:

1. **Production Figures Accuracy**: Cross-check ROM, Product, and equipment data against original WhatsApp sources
2. **Equipment Breakdown Details**: Verify breakdown descriptions match source data
3. **Site Assignment**: Confirm reports are assigned to correct sites and engineers
4. **Mathematical Consistency**: Check variance calculations and percentages

### ğŸš¨ Important Notes

- **Manual Review Recommended**: While automated processing provides good structure, human verification of production figures is essential
- **Source Data Available**: Original WhatsApp extractions included for traceability
- **Gemini Processing**: Uses template-compliant prompts with strict data extraction requirements
- **No Data Fabrication**: System instructed to use "null" for missing data rather than estimates

### ğŸ“‹ Approval Checklist

- [ ] Production figures verified against WhatsApp sources
- [ ] Equipment data accuracy confirmed  
- [ ] File structure and naming conventions correct
- [ ] Markdown formatting and links functional
- [ ] JSON schema compliance validated
- [ ] No sensitive information exposed

### ğŸ”„ Next Steps After Approval

1. Merge to main branch
2. Update processing templates if needed
3. Scale to additional date ranges (July 6-21)
4. Implement daily automation workflow

---

ğŸ¤– **Automated PR created by GitHub Actions + Gemini Integration**  
ğŸ“… **Generated**: $(date)  
ğŸ”§ **Workflow**: `.github/workflows/create-report-pr.yml`

EOF
)" \
          --base "${{ steps.params.outputs.target_branch }}" \
          --head "${{ steps.params.outputs.branch_name }}" \
          --assignee "${{ github.actor }}"
        
    - name: Create Processing Summary
      run: |
        echo "ğŸ“‹ Processing complete! Summary:"
        echo "================================"
        echo "ğŸ“… Date processed: ${{ steps.params.outputs.date }}"
        echo "ğŸŒ¿ Branch created: ${{ steps.params.outputs.branch_name }}"
        echo "ğŸ“ JSON files: $(find daily_production -name "*${{ steps.params.outputs.date }}*.json" | wc -l)"
        echo "ğŸ“„ Markdown files: $(find daily_production -name "*${{ steps.params.outputs.date }}*.md" | wc -l)"
        echo "ğŸ”— Pull request: Created and ready for review"
        echo ""
        echo "ğŸ¯ Next: Review the PR, validate data accuracy, and merge when ready"