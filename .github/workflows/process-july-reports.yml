name: Process Missing Production Reports (July 6-21, 2025)

on:
  issues:
    types: [opened, reopened, labeled]
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start date (YYYY-MM-DD)'
        required: true
        default: '2025-07-06'
      end_date:
        description: 'End date (YYYY-MM-DD)'
        required: true
        default: '2025-07-21'

jobs:
  process-reports:
    if: contains(github.event.issue.labels.*.name, 'final-july6') || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests sqlite3-tools python-dateutil
        
    - name: Configure date range
      id: dates
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "start_date=${{ github.event.inputs.start_date }}" >> $GITHUB_OUTPUT
          echo "end_date=${{ github.event.inputs.end_date }}" >> $GITHUB_OUTPUT
        else
          echo "start_date=2025-07-06" >> $GITHUB_OUTPUT
          echo "end_date=2025-07-06" >> $GITHUB_OUTPUT
        fi
        
    - name: Start and connect to Codespace
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      run: |
        echo "Connecting to Codespace for WhatsApp data access..."
        # Check if Codespace exists and start if needed
        CODESPACE_NAME="cuddly-guacamole-496vp6p46wg39r"
        echo "codespace_name=$CODESPACE_NAME" >> $GITHUB_ENV
        
    - name: Extract WhatsApp data
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      run: |
        echo "Extracting WhatsApp production data for date range..."
        
        # Create data extraction script
        cat > extract_whatsapp_data.py << 'EOF'
        import subprocess
        import json
        import sys
        from datetime import datetime, timedelta
        
        def extract_production_messages(start_date, end_date):
            # Convert dates to timestamp format for SQLite query
            start_ts = f"{start_date} 00:00:00+00:00"
            end_ts = f"{end_date} 23:59:59+00:00"
            
            # SQL query to extract production messages
            sql_query = f"SELECT timestamp,chat_jid,sender,content FROM messages WHERE (chat_jid = '27834418149-1537194373@g.us' OR chat_jid = '120363204285087803@g.us') AND timestamp BETWEEN '{start_ts}' AND '{end_ts}' AND (content LIKE '%ROM%' OR content LIKE '%Safety%' OR content LIKE '%Production%' OR content LIKE '%Gloria%' OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%' OR content LIKE '%Shafts%') ORDER BY timestamp ASC"
            
            # Execute query via Codespace SSH
            codespace_name = sys.argv[1] if len(sys.argv) > 1 else "cuddly-guacamole-496vp6p46wg39r"
            
            cmd = ['gh', 'codespace', 'ssh', '-c', codespace_name, '--', 'sqlite3', '/workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db', sql_query]
            
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                return result.stdout
            except subprocess.CalledProcessError as e:
                print(f"Error executing query: {e}")
                print(f"STDERR: {e.stderr}")
                return None
        
        if __name__ == "__main__":
            start_date = "${{ steps.dates.outputs.start_date }}"
            end_date = "${{ steps.dates.outputs.end_date }}"
            
            print(f"Extracting data from {start_date} to {end_date}")
            data = extract_production_messages(start_date, end_date)
            
            if data:
                with open('raw_whatsapp_data.txt', 'w') as f:
                    f.write(data)
                print(f"Extracted {len(data.splitlines())} lines of WhatsApp data")
            else:
                print("Failed to extract WhatsApp data")
                sys.exit(1)
        EOF
        
        python extract_whatsapp_data.py ${{ env.codespace_name }}
        
    - name: Process reports with Gemini-CLI simulation
      run: |
        echo "Processing reports sequentially..."
        
        # Create processing script that simulates Gemini-CLI behavior
        cat > process_reports.py << 'EOF'
        import re
        import json
        from datetime import datetime
        from pathlib import Path
        
        def parse_whatsapp_messages(raw_data):
            """Parse raw WhatsApp data into structured messages"""
            messages = []
            lines = raw_data.strip().split('\n')
            
            for line in lines:
                if '|' in line:
                    parts = line.split('|', 3)
                    if len(parts) >= 4:
                        timestamp = parts[0]
                        chat_jid = parts[1] 
                        sender = parts[2]
                        content = parts[3]
                        
                        messages.append({
                            'timestamp': timestamp,
                            'chat_jid': chat_jid,
                            'sender': sender,
                            'content': content
                        })
            
            return messages
        
        def identify_site_from_content(content):
            """Identify mine site from message content"""
            content_lower = content.lower()
            
            if 'gloria' in content_lower:
                return 'Gloria'
            elif 'nchwaning 3' in content_lower or 'n3' in content_lower:
                return 'Nchwaning 3'  
            elif 'nchwaning 2' in content_lower or 'n2' in content_lower:
                return 'Nchwaning 2'
            elif 's&w' in content_lower or 'shafts' in content_lower:
                return 'Shafts & Winders'
            
            return 'Unknown'
        
        def extract_date_from_content(content, timestamp):
            """Extract report date from content or use timestamp"""
            # Look for date patterns in content
            date_patterns = [
                r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',
                r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, content)
                if match:
                    # Return first found date
                    return match.group(0)
            
            # Fallback to timestamp date
            return timestamp.split(' ')[0] if ' ' in timestamp else timestamp
        
        def create_report_entry(message, site, report_date):
            """Create a report entry for GitHub processing"""
            return {
                'timestamp': message['timestamp'],
                'site': site,
                'report_date': report_date,
                'content': message['content'],
                'raw_message': message
            }
        
        # Main processing
        if __name__ == "__main__":
            with open('raw_whatsapp_data.txt', 'r') as f:
                raw_data = f.read()
            
            messages = parse_whatsapp_messages(raw_data)
            reports = []
            
            for msg in messages:
                site = identify_site_from_content(msg['content'])
                if site != 'Unknown':
                    report_date = extract_date_from_content(msg['content'], msg['timestamp'])
                    report = create_report_entry(msg, site, report_date)
                    reports.append(report)
            
            # Group by date and site
            reports_by_date = {}
            for report in reports:
                date_key = report['report_date']
                if date_key not in reports_by_date:
                    reports_by_date[date_key] = []
                reports_by_date[date_key].append(report)
            
            # Save structured data for next step
            with open('structured_reports.json', 'w') as f:
                json.dump(reports_by_date, f, indent=2)
            
            print(f"Processed {len(reports)} reports across {len(reports_by_date)} dates")
            print("Dates found:", list(reports_by_date.keys()))
        EOF
        
        python process_reports.py
        
    - name: Create report files
      run: |
        echo "Creating production report files..."
        
        # Create file generation script
        cat > create_files.py << 'EOF'
        import json
        import os
        from pathlib import Path
        
        def create_production_files(reports_data):
            """Create production report files"""
            
            # Ensure directories exist
            Path("daily_production").mkdir(exist_ok=True)
            Path("daily_production/data").mkdir(exist_ok=True)
            
            created_files = []
            
            for date, reports in reports_data.items():
                for report in reports:
                    site = report['site'].replace(' ', '_').lower()
                    timestamp = report['timestamp']
                    content = report['content']
                    
                    # Create JSON data file
                    json_filename = f"daily_production/data/{date}_{site}.json"
                    json_data = {
                        "report_metadata": {
                            "date": date,
                            "site": report['site'],
                            "timestamp": timestamp,
                            "source": "WhatsApp Production Group",
                            "processing": "Automated via GitHub Actions"
                        },
                        "raw_content": content,
                        "processing_note": "This report was automatically processed from WhatsApp data. Manual review recommended."
                    }
                    
                    with open(json_filename, 'w') as f:
                        json.dump(json_data, f, indent=2)
                    
                    created_files.append(json_filename)
                    
                    # Create Markdown report file  
                    md_filename = f"daily_production/{date} – {report['site']} Daily Report.md"
                    md_content = f"""---
JSONData:: [[{date}_{site}.json]]
---

# {report['site']} Daily Report
**Date**: {date}  
**Source**: WhatsApp Production Group
**Processing**: Automated via GitHub Actions

## 🤖 AUTOMATED PROCESSING - MANUAL REVIEW REQUIRED

**Raw WhatsApp Content:**
```
{content}
```

**Processing Notes:**
- This report was automatically extracted from WhatsApp messages
- Content requires manual parsing and validation
- Equipment data, production figures, and safety information need verification
- Please process using /pdr-single command for proper structuring

**Next Steps:**
1. Review raw content above
2. Apply proper production report template
3. Validate all data against source
4. Update with structured JSON data
"""
                    
                    with open(md_filename, 'w') as f:
                        f.write(md_content)
                        
                    created_files.append(md_filename)
            
            return created_files
        
        if __name__ == "__main__":
            with open('structured_reports.json', 'r') as f:
                reports_data = json.load(f)
            
            created_files = create_production_files(reports_data)
            
            # Save list of created files
            with open('created_files.txt', 'w') as f:
                for file in created_files:
                    f.write(f"{file}\n")
                    
            print(f"Created {len(created_files)} files")
        EOF
        
        python create_files.py
        
    - name: Create Pull Request
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: |
          feat: Add automated production reports for July 6-21, 2025
          
          - Extracted WhatsApp production data for date range
          - Created placeholder report files for manual processing  
          - Generated JSON data files with raw content
          - Requires manual review and /pdr-single processing
          
          🤖 Generated via GitHub Actions automation
          📅 Date Range: ${{ steps.dates.outputs.start_date }} to ${{ steps.dates.outputs.end_date }}
        title: 'Production Reports: Automated Extraction July 6-21, 2025'
        body: |
          ## Automated Production Report Extraction
          
          **Date Range**: ${{ steps.dates.outputs.start_date }} to ${{ steps.dates.outputs.end_date }}
          **Processing Method**: GitHub Actions → WhatsApp Data Extraction
          
          ### What This PR Contains
          - Raw WhatsApp production data extracted from database
          - Placeholder report files requiring manual processing
          - JSON data files with metadata and raw content
          
          ### Next Steps Required
          1. **Review extracted data** for completeness and accuracy
          2. **Apply /pdr-single processing** to structure reports properly
          3. **Validate equipment codes** and production figures
          4. **Update with proper templates** (Standard Mine Site / S&W)
          
          ### Files Created
          See `created_files.txt` for complete list of generated files.
          
          ### Manual Processing Required
          Each report file contains raw WhatsApp content that needs:
          - Safety status extraction and validation
          - Production metrics parsing (ROM, Product, Loads)  
          - Equipment availability calculations
          - Breakdown analysis with unit numbers
          - Template formatting according to site standards
          
          **⚠️ IMPORTANT**: All data should be verified against original WhatsApp sources before final approval.
          
          🤖 This PR was generated automatically by GitHub Actions.
        branch: feature/july-reports-automation
        delete-branch: true
        
    - name: Comment on issue (if triggered by issue)
      if: github.event_name == 'issues'
      uses: peter-evans/create-or-update-comment@v3
      with:
        issue-number: ${{ github.event.issue.number }}
        body: |
          ## ✅ Processing Complete
          
          Automated processing of production reports has completed successfully.
          
          **Date Range Processed**: ${{ steps.dates.outputs.start_date }} to ${{ steps.dates.outputs.end_date }}
          
          **Pull Request Created**: A pull request has been created with the extracted WhatsApp data and placeholder report files.
          
          **Next Steps**:
          1. Review the pull request for data accuracy
          2. Apply manual processing using /pdr-single commands  
          3. Validate and approve reports for integration
          
          The automation has successfully extracted the raw production data. Manual review and processing is now required to complete the report structuring.