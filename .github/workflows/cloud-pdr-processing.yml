name: Autonomous Cloud PDR Processing

on:
  repository_dispatch:
    types: [pdr-cloud]

jobs:
  autonomous-pdr-processing:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
      pull-requests: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Check Bridge Health & Extract WhatsApp Messages
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: extract
      run: |
        CODESPACE_NAME="cuddly-guacamole-496vp6p46wg39r"
        TARGET_DATE="${{ github.event.client_payload.date }}"
        ADDITIONAL_INSTRUCTIONS="${{ github.event.client_payload.instructions }}"
        
        echo "üîç Checking WhatsApp bridge health and extracting messages for $TARGET_DATE..."
        
        # Check if bridge service is running and restart if needed (with retry logic)
        echo "üîß Checking bridge service status..."
        BRIDGE_RETRY_COUNT=0
        MAX_BRIDGE_RETRIES=3
        BRIDGE_SUCCESS=false
        
        while [ $BRIDGE_RETRY_COUNT -lt $MAX_BRIDGE_RETRIES ] && [ "$BRIDGE_SUCCESS" = false ]; do
          BRIDGE_STATUS=$(gh codespace ssh -c $CODESPACE_NAME -- \
            "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh status" 2>/dev/null || echo "Bridge not running")
          
          if [[ "$BRIDGE_STATUS" == *"not running"* ]] || [[ "$BRIDGE_STATUS" == *"error"* ]]; then
            echo "üö® Bridge not running (attempt $((BRIDGE_RETRY_COUNT + 1))/$MAX_BRIDGE_RETRIES) - starting service..."
            gh codespace ssh -c $CODESPACE_NAME -- \
              "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh restart"
            sleep 15  # Give bridge more time to start properly
            BRIDGE_RETRY_COUNT=$((BRIDGE_RETRY_COUNT + 1))
          else
            echo "‚úÖ Bridge service is running"
            BRIDGE_SUCCESS=true
          fi
        done
        
        if [ "$BRIDGE_SUCCESS" = false ]; then
          echo "‚ùå Failed to start bridge after $MAX_BRIDGE_RETRIES attempts"
          echo "üîç This may indicate Codespace connectivity issues or bridge configuration problems"
        fi
        
        # Extract messages from Codespace SQLite database (with retry and error handling)
        echo "üìã Extracting messages with production keywords..."
        EXTRACTION_SUCCESS=false
        EXTRACTION_RETRIES=0
        MAX_EXTRACTION_RETRIES=2
        
        while [ $EXTRACTION_RETRIES -lt $MAX_EXTRACTION_RETRIES ] && [ "$EXTRACTION_SUCCESS" = false ]; do
          if gh codespace ssh -c $CODESPACE_NAME -- \
            "sqlite3 /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db \"SELECT timestamp, chat_jid, sender, content FROM messages WHERE timestamp BETWEEN '${TARGET_DATE} 00:00:00+00:00' AND '${TARGET_DATE} 23:59:59+00:00' AND chat_jid = '27834418149-1537194373@g.us' AND (content LIKE '%ROM%' OR content LIKE '%Safety%' OR content LIKE '%Gloria Report%' OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%' OR content LIKE '%Decline%' OR content LIKE '%Product%' OR content LIKE '%Loads%') ORDER BY timestamp;\"" > extracted_messages.txt 2>&1; then
            echo "‚úÖ Data extraction completed successfully"
            EXTRACTION_SUCCESS=true
          else
            echo "‚ùå Data extraction failed (attempt $((EXTRACTION_RETRIES + 1))/$MAX_EXTRACTION_RETRIES)"
            EXTRACTION_RETRIES=$((EXTRACTION_RETRIES + 1))
            if [ $EXTRACTION_RETRIES -lt $MAX_EXTRACTION_RETRIES ]; then
              echo "üîÑ Retrying data extraction in 10 seconds..."
              sleep 10
            fi
          fi
        done
        
        if [ "$EXTRACTION_SUCCESS" = false ]; then
          echo "‚ùå CRITICAL: Data extraction failed after $MAX_EXTRACTION_RETRIES attempts"
          echo "üîç Check Codespace connectivity and database accessibility"
          exit 1
        fi
        
        # Debug: Check if file was created and has content
        if [ -f extracted_messages.txt ]; then
          echo "‚úÖ Extraction file created"
          echo "üìÑ File size: $(stat -c%s extracted_messages.txt 2>/dev/null || stat -f%z extracted_messages.txt 2>/dev/null || echo 'unknown') bytes"
        else
          echo "‚ùå Extraction file not created"
        fi
           
        # Count messages and validate extraction (exclude empty file or header-only cases)
        if [ -s extracted_messages.txt ]; then
          # File exists and has content - count actual data lines
          MESSAGE_COUNT=$(grep -c '|' extracted_messages.txt || echo "0")
          echo "üìä Found $MESSAGE_COUNT WhatsApp messages for $TARGET_DATE"
        else
          # File is empty or doesn't exist
          MESSAGE_COUNT=0
          echo "üìä Found 0 WhatsApp messages for $TARGET_DATE (empty extraction)"
        fi
        
        # Log if no data found (but don't exit - let conditional handle it)
        if [ "$MESSAGE_COUNT" -eq 0 ]; then
          echo "‚ö†Ô∏è  No production messages found for $TARGET_DATE"
          echo "Data may not be available or date may be incorrect"
          echo "üîç Debug: File size $(stat -c%s extracted_messages.txt 2>/dev/null || echo '0') bytes"
        fi
        
        # Output variables for next steps
        echo "message_count=$MESSAGE_COUNT" >> $GITHUB_OUTPUT
        echo "target_date=$TARGET_DATE" >> $GITHUB_OUTPUT
        
        # Preview extracted data (first 3 lines)
        echo "üìã Message preview:"
        head -3 extracted_messages.txt || echo "No data to preview"
    
    - name: Handle No Data Found
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: steps.extract.outputs.message_count == '0'
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        echo "üõë STOPPING PROCESSING - No data available for $TARGET_DATE"
        echo "Possible reasons:"
        echo "  - Date may be a weekend/holiday with no reports"
        echo "  - Bridge may have been offline during report time"
        echo "  - Reports may use different keywords not in search filter"
        echo "  - Date format may be incorrect (use YYYY-MM-DD)"
        
        # Create a no-data notification issue
        gh issue create \
          --title "‚ùå No Data Found: $TARGET_DATE" \
          --body "**No production data available for $TARGET_DATE**

        üìä **Search Results**: 0 messages found
        üîç **Search Criteria**: ROM, Production, Gloria, Nchwaning, S&W keywords
        ‚è∞ **Time Range**: Full day (00:00-23:59 UTC)
        
        **Possible Reasons**:
        - Weekend/holiday - no production reports sent
        - WhatsApp bridge offline during report time (4:00-6:00 AM SAST)
        - Engineers used different messaging keywords
        - Date format incorrect or date does not exist
        
        **Bridge Status**: Checked and restarted if needed
        **Recommendation**: Verify date and retry, or process manually if reports exist" \
          --label "no-data" \
          --label "pdr-cloud"
        
        echo "‚úÖ Created no-data notification issue"
        
    - name: Create Processing Issue
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: issue
      if: steps.extract.outputs.message_count > 0
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        echo "üìù Creating processing issue for autonomous Claude processing..."
        
        # Debug: Show what data we're about to include
        echo "üîç Data to include in issue (first 5 lines):"
        head -5 extracted_messages.txt || echo "No data to preview"
        echo "üìÑ Full data file content:"
        cat extracted_messages.txt | wc -l
        
        # Create issue with extracted data and @claude mention
        ISSUE_URL=$(gh issue create \
          --title "ü§ñ Autonomous PDR Processing - $TARGET_DATE" \
          --label "autonomous-processing" \
          --label "pdr-cloud" \
          --body "$(cat <<EOF
        @claude Please process these daily production reports autonomously:

        **Processing Request Details:**
        - **Date**: $TARGET_DATE
        - **Messages Found**: $MESSAGE_COUNT WhatsApp messages
        - **Processing Mode**: Fully Autonomous
        - **Triggered From**: /pdr-cloud command in local Claude Code

        **Processing Instructions:**
        1. Follow /pdr-single workflow logic exactly
        2. Use appropriate Report Templates:
           - Standard Mine Site Report Template ‚Üí Gloria, Nchwaning 2, Nchwaning 3  
           - Shafts & Winders Report Template ‚Üí Infrastructure reports
        3. Create both JSON and Markdown files for each detected site
        4. Use exact folder structure: \`daily_production/data/YYYY-MM/DD/\`
        5. Include proper engineer assignment based on site
        6. Add source validation sections in JSON files
        7. **CRITICAL: Create Pull Request immediately** after generating files - do not provide links, actually create the PR
        8. **Use this exact PR creation command after committing files**:
           \`gh pr create --title \"feat: Autonomous PDR processing for $TARGET_DATE\" --body \"Auto-processed daily production reports\" --head [your-branch] --base master\`
        9. **This issue will auto-close** after successful processing
        
        $(if [ -n "$ADDITIONAL_INSTRUCTIONS" ]; then
        echo "**üéØ ADDITIONAL PROCESSING INSTRUCTIONS:**"
        echo "$ADDITIONAL_INSTRUCTIONS"
        echo ""
        echo "**IMPORTANT**: Follow the above additional instructions while still adhering to all standard processing requirements."
        fi)

        **Expected Sites**: Gloria, Nchwaning 2, Nchwaning 3, Shafts & Winders

        **Raw WhatsApp Data:**
        \`\`\`
        $(cat extracted_messages.txt)
        \`\`\`

        **Auto-Processing Request**: Please process and create PR with results immediately. The workflow will handle auto-merge and issue closure.
        EOF
        )")
        
        # Extract issue number from URL
        ISSUE_NUMBER=$(echo $ISSUE_URL | grep -o '[0-9]*$')
        echo "‚úÖ Created processing issue #$ISSUE_NUMBER"
        echo "üîó Issue URL: $ISSUE_URL"
        
        # Output variables for monitoring steps
        echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
        echo "issue_url=$ISSUE_URL" >> $GITHUB_OUTPUT
        
    - name: Monitor Claude Processing
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: monitor
      if: steps.extract.outputs.message_count > 0
      run: |
        ISSUE_NUMBER="${{ steps.issue.outputs.issue_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        
        echo "üëÄ Monitoring Claude processing for issue #$ISSUE_NUMBER"
        echo "‚è≥ Waiting for Claude to process reports and create PR..."
        
        # Monitor for PR creation (max 30 minutes) with improved error handling
        TIMEOUT=120  # 120 iterations * 15 seconds = 30 minutes
        PR_FOUND=false
        CHECK_FAILURES=0
        MAX_CHECK_FAILURES=5
        
        for i in $(seq 1 $TIMEOUT); do
          sleep 15
          
          # Check if Claude has created a PR for this specific date (avoid jq dependency)
          if PR_LIST=$(gh pr list --state open --json number,title,createdAt 2>/dev/null); then
            # Reset failure counter on successful API call
            CHECK_FAILURES=0
            
            # Search for PR with target date (without jq)
            if echo "$PR_LIST" | grep -q "$TARGET_DATE" && (echo "$PR_LIST" | grep -q "PDR\|production\|Autonomous"); then
              # Extract PR number from the matching line
              PR_NUMBER=$(echo "$PR_LIST" | grep "$TARGET_DATE" | grep -o '"number":[0-9]*' | head -1 | grep -o '[0-9]*')
              if [ -n "$PR_NUMBER" ]; then
                PR_TITLE=$(gh pr view $PR_NUMBER --json title --template '{{.title}}' 2>/dev/null || echo "PR #$PR_NUMBER")
                echo "‚úÖ Claude created PR #$PR_NUMBER: $PR_TITLE"
                echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
                PR_FOUND=true
                break
              fi
            fi
          else
            # Handle API failures gracefully
            CHECK_FAILURES=$((CHECK_FAILURES + 1))
            echo "‚ö†Ô∏è  GitHub API check failed ($CHECK_FAILURES/$MAX_CHECK_FAILURES)"
            
            if [ $CHECK_FAILURES -ge $MAX_CHECK_FAILURES ]; then
              echo "‚ùå Too many API failures - monitoring may be unreliable"
              echo "üîç Continuing with extended timeout but check issue #$ISSUE_NUMBER manually"
            fi
          fi
          
          # Show progress every minute
          if [ $((i % 4)) -eq 0 ]; then
            echo "‚è≥ Still waiting for Claude processing... ($((i*15))s elapsed)"
            echo "üîç Check issue #$ISSUE_NUMBER for current progress"
          fi
        done
        
        if [ "$PR_FOUND" = false ]; then
          echo "‚ùå Timeout: No PR created within 30 minutes"
          echo "üîç Check issue #$ISSUE_NUMBER for Claude's progress"
          echo "üí° This may indicate Claude processing is taking longer than expected"
          echo "üìã Issue will remain open for manual completion if needed"
          exit 1
        fi
        
    - name: Auto-Validate and Auto-Merge PR
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: merge
      if: steps.extract.outputs.message_count > 0
      run: |
        PR_NUMBER="${{ steps.monitor.outputs.pr_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        echo "üîç Auto-validating PR #$PR_NUMBER for merge..."
        
        # Checkout the PR to inspect files
        gh pr checkout $PR_NUMBER
        
        # Calculate expected directory structure
        YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
        DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
        EXPECTED_DIR="daily_production/data/${YEAR_MONTH}/${DAY}"
        
        echo "üìÅ Checking for files in: $EXPECTED_DIR"
        
        # Validation checks
        VALIDATION_PASSED=true
        VALIDATION_MESSAGES=""
        
        # Check 1: Expected directory exists
        if [ -d "$EXPECTED_DIR" ]; then
          echo "‚úÖ Directory structure correct: $EXPECTED_DIR"
        else
          echo "‚ùå Expected directory not found: $EXPECTED_DIR"
          VALIDATION_PASSED=false
          VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- Missing directory: $EXPECTED_DIR"
        fi
        
        # Check 2: Validate paired JSON+Markdown files for each site
        if [ -d "$EXPECTED_DIR" ]; then
          JSON_COUNT=$(find "$EXPECTED_DIR" -name "*.json" | wc -l)
          MD_COUNT=$(find "$EXPECTED_DIR" -name "*.md" | wc -l)
          TOTAL_FILES=$((JSON_COUNT + MD_COUNT))
          
          echo "üìä Found $JSON_COUNT JSON files and $MD_COUNT Markdown files"
          
          # Check for paired files - each JSON should have matching Markdown
          JSON_SITES=$(find "$EXPECTED_DIR" -name "*_*.json" -exec basename {} \; | sed 's/.*_\(.*\)\.json/\1/' | sort)
          MD_SITES=$(find "$EXPECTED_DIR" -name "*Daily Report.md" -o -name "*Weekly Report.md" | wc -l)
          
          # Verify each site has both JSON and Markdown
          MISSING_PAIRS=""
          for site in gloria nchwaning2 nchwaning3 shafts_winders; do
            JSON_FILE="$EXPECTED_DIR/${TARGET_DATE}_${site}.json"
            if [ -f "$JSON_FILE" ]; then
              # Check for corresponding markdown file
              MD_EXISTS=false
              for md_file in "$EXPECTED_DIR"/*Daily\ Report.md "$EXPECTED_DIR"/*Weekly\ Report.md; do
                if [ -f "$md_file" ]; then
                  case "$site" in
                    "gloria")
                      if echo "$md_file" | grep -q -i "gloria"; then MD_EXISTS=true; break; fi
                      ;;
                    "nchwaning2")
                      if echo "$md_file" | grep -q -i "nchwaning.*2"; then MD_EXISTS=true; break; fi
                      ;;
                    "nchwaning3")
                      if echo "$md_file" | grep -q -i "nchwaning.*3"; then MD_EXISTS=true; break; fi
                      ;;
                    "shafts_winders")
                      if echo "$md_file" | grep -q -i "shafts.*winders"; then MD_EXISTS=true; break; fi
                      ;;
                  esac
                fi
              done
              if [ "$MD_EXISTS" = false ]; then
                MISSING_PAIRS="$MISSING_PAIRS $site"
                echo "‚ùå Missing Markdown report for site: $site (JSON exists)"
              else
                echo "‚úÖ Complete pair for site: $site"
              fi
            fi
          done
          
          if [ -n "$MISSING_PAIRS" ]; then
            echo "‚ùå CRITICAL: Missing Markdown reports for sites:$MISSING_PAIRS"
            VALIDATION_PASSED=false
            VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- Missing Markdown reports for:$MISSING_PAIRS"
            VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- This indicates Claude Cloud partial failure - JSON generated but Markdown creation failed"
          else
            echo "‚úÖ All sites have complete JSON+Markdown pairs"
          fi
        else
          TOTAL_FILES=0
        fi
        
        # Check 3: File location validation (no files in wrong locations)
        MISPLACED_FILES=$(find "daily_production/" -maxdepth 1 -name "*$TARGET_DATE*" -type f | wc -l)
        if [ "$MISPLACED_FILES" -eq 0 ]; then
          echo "‚úÖ No misplaced files in daily_production root"
        else
          echo "‚ùå Found $MISPLACED_FILES files in wrong location (daily_production/ root)"
          VALIDATION_PASSED=false
          VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- $MISPLACED_FILES files in wrong location (should be in data/YYYY-MM/DD/)"
          # List the misplaced files for debugging
          find "daily_production/" -maxdepth 1 -name "*$TARGET_DATE*" -type f | while read file; do
            echo "  ‚ùå Misplaced: $file"
          done
        fi
        
        # Check 4: File size validation (not empty, reasonable size)
        if [ "$TOTAL_FILES" -gt 0 ]; then
          EMPTY_FILES=$(find "$EXPECTED_DIR" \( -name "*.json" -o -name "*.md" \) -empty | wc -l)
          if [ "$EMPTY_FILES" -eq 0 ]; then
            echo "‚úÖ No empty files detected"
          else
            echo "‚ùå Found $EMPTY_FILES empty files"
            VALIDATION_PASSED=false
            VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- $EMPTY_FILES empty files detected"
          fi
        fi
        
        # Check 5: Site identification validation (check for known phone number mappings)
        if [ -d "$EXPECTED_DIR" ]; then
          echo "üîç Validating site identification against known phone mappings..."
          # Check if site assignments match expected engineer assignments
          SITE_ERRORS=0
          
          # Validate N2 vs N3 confusion (common error pattern)
          if [ -f "$EXPECTED_DIR"/*nchwaning2* ] && [ -f "$EXPECTED_DIR"/*nchwaning3* ]; then
            echo "‚ö†Ô∏è  WARNING: Both N2 and N3 files detected - verify site identification"
          fi
          
          # Note: Full phone number validation would require access to the raw WhatsApp data
          # This check focuses on catching obvious site misidentification patterns
          echo "‚úÖ Site identification check completed"
        fi
        
        # Proceed with merge if validation passed
        if [ "$VALIDATION_PASSED" = true ]; then
          echo "üéØ All validations passed - proceeding with auto-merge"
          
          # Auto-merge PR with descriptive commit message
          gh pr merge $PR_NUMBER --squash --delete-branch \
            --subject "feat: Autonomous daily production reports - $TARGET_DATE" \
            --body "Auto-processed and auto-merged production reports for $TARGET_DATE

        üìä **Processing Summary:**
        - Extracted $MESSAGE_COUNT WhatsApp messages from Codespace
        - Generated $TOTAL_FILES report files ($JSON_COUNT JSON, $MD_COUNT Markdown)
        - Applied Report Templates for standardized formatting
        - Validated file structure and content automatically
        - Completed autonomous processing via /pdr-cloud command

        üìÅ **Files Location:** \`daily_production/data/$YEAR_MONTH/$DAY/\`

        ü§ñ **Fully autonomous processing** - no manual intervention required
        ‚úÖ **Auto-validation passed** - all quality checks successful"
          
          echo "‚úÖ PR #$PR_NUMBER auto-merged successfully"
          echo "files_created=$TOTAL_FILES" >> $GITHUB_OUTPUT
          
        else
          echo "‚ùå Validation failed - PR will not be auto-merged"
          echo "üîç Validation issues:$VALIDATION_MESSAGES"
          echo "üìã Manual review required for PR #$PR_NUMBER"
          exit 1
        fi
        
    - name: Auto-Close Issue with Success Summary  
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: success() && steps.extract.outputs.message_count > 0
      run: |
        ISSUE_NUMBER="${{ steps.issue.outputs.issue_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        FILES_CREATED="${{ steps.merge.outputs.files_created }}"
        PR_NUMBER="${{ steps.monitor.outputs.pr_number }}"
        
        echo "üéØ Auto-closing issue #$ISSUE_NUMBER with success summary"
        
        # Close issue with comprehensive summary
        gh issue close $ISSUE_NUMBER \
          --comment "‚úÖ **Autonomous Processing Successfully Completed**

        ## üìä Processing Summary
        - **Date Processed**: $TARGET_DATE
        - **WhatsApp Messages**: $MESSAGE_COUNT messages extracted
        - **Files Generated**: $FILES_CREATED files (JSON + Markdown)
        - **PR Created**: #$PR_NUMBER (auto-merged)
        - **Processing Time**: Complete workflow in under 10 minutes

        ## üéØ Workflow Steps Completed
        - ‚úÖ WhatsApp data extracted from Codespace SQLite database
        - ‚úÖ Claude processed reports using /pdr-single workflow logic  
        - ‚úÖ Applied appropriate Report Templates for each site
        - ‚úÖ Generated structured JSON and readable Markdown files
        - ‚úÖ Validated file structure and content automatically
        - ‚úÖ Auto-merged PR after successful validation
        - ‚úÖ Auto-closed issue with this summary

        ## üìÅ Results Available
        **Location**: \`daily_production/data/$(echo $TARGET_DATE | cut -d'-' -f1,2)/$(echo $TARGET_DATE | cut -d'-' -f3)/\`
        **Access**: Run \`git pull\` to sync files locally

        ## üöÄ Automation Details
        **Triggered**: Via \`/pdr-cloud $TARGET_DATE\` command from local Claude Code
        **Processing**: Fully autonomous - no manual intervention required
        **Quality**: Auto-validated against established templates and structure

        ---
        ü§ñ **Issue auto-closed** - autonomous processing pipeline complete"
        
        echo "‚úÖ Issue #$ISSUE_NUMBER closed with success summary"
        echo "üéâ Autonomous processing pipeline completed successfully!"
        
    - name: Auto-Push to Local Repository
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: success() && steps.extract.outputs.message_count > 0
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        FILES_CREATED="${{ steps.merge.outputs.files_created }}"
        
        echo "üîÑ Initiating automatic push to local repository..."
        
        # Send repository dispatch to trigger local sync
        echo "üì° Sending auto-sync dispatch to local repository..."
        echo '{"event_type": "auto-sync", "client_payload": {"date": "'$TARGET_DATE'", "files_created": "'$FILES_CREATED'", "trigger": "cloud-processing-complete"}}' | gh api repos/karstegg/MarthaVault/dispatches --method POST --input -
        
        if [ $? -eq 0 ]; then
          echo "‚úÖ Auto-sync dispatch sent successfully"
          echo "üîÑ Local repository will automatically update with new files"
          
          # Create notification issue for confirmation
          gh issue create \
            --title "üîÑ Auto-Push Complete: $TARGET_DATE" \
            --body "‚úÖ **Automatic Push to Local Repository Initiated**

          ## üìä Processing Complete
          - **Date**: $TARGET_DATE
          - **Files Created**: $FILES_CREATED files
          - **Status**: ‚úÖ Auto-merged and pushed to local

          ## üöÄ Auto-Push Details
          - **Trigger**: Repository dispatch sent to local environment
          - **Action**: New files automatically pushed to local repository
          - **Safety**: Only new/changed files affected, existing local files untouched

          ## üìÅ Files Now Available Locally
          Location: \`daily_production/data/$(echo $TARGET_DATE | cut -d'-' -f1,2)/$(echo $TARGET_DATE | cut -d'-' -f3)/\`
          
          **Files**:
          - JSON database files for analysis
          - Markdown reports for review
          - All properly organized in daily_production structure

          ---
          ü§ñ **Fully Autonomous** - Files automatically available in your local repository" \
            --label "auto-push" \
            --label "completed"
          
          echo "üìß Auto-push confirmation notification created"
        else
          echo "‚ùå Failed to send auto-sync dispatch"
          echo "üîç Check repository dispatch configuration"
        fi
        
    - name: Handle Processing Failures
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: failure()
      run: |
        ISSUE_NUMBER="${{ steps.issue.outputs.issue_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        # Determine failure category based on available information
        if [ -z "$TARGET_DATE" ]; then
          FAILURE_CATEGORY="Configuration Error"
          FAILURE_DETAILS="Workflow configuration or GitHub Actions setup issue"
        elif [ -z "$MESSAGE_COUNT" ] || [ "$MESSAGE_COUNT" = "0" ]; then
          FAILURE_CATEGORY="Data Extraction Error"
          FAILURE_DETAILS="WhatsApp bridge connectivity or database access issue"
        elif [ -z "$ISSUE_NUMBER" ]; then
          FAILURE_CATEGORY="Issue Creation Error" 
          FAILURE_DETAILS="GitHub API permissions or repository access issue"
        else
          FAILURE_CATEGORY="Processing Timeout"
          FAILURE_DETAILS="Claude processing or PR creation timeout"
        fi
        
        echo "‚ùå Autonomous processing failed: $FAILURE_CATEGORY"
        echo "üîç Details: $FAILURE_DETAILS"
        
        if [ -n "$ISSUE_NUMBER" ]; then
          echo "üìù Updating issue #$ISSUE_NUMBER with failure details..."
          
          # Create comprehensive failure report
          gh issue comment $ISSUE_NUMBER \
            --body "‚ùå **Autonomous Processing Failed - $FAILURE_CATEGORY**

          ## üìä Failure Summary
          - **Date**: $TARGET_DATE  
          - **Category**: $FAILURE_CATEGORY
          - **Details**: $FAILURE_DETAILS
          - **Messages Found**: ${MESSAGE_COUNT:-'Unknown'}
          - **Workflow Run**: [View Logs](https://github.com/karstegg/MarthaVault/actions/runs/$GITHUB_RUN_ID)
          
          ## üîç Troubleshooting Guide
          
          ### $FAILURE_CATEGORY
          $(if [ "$FAILURE_CATEGORY" = "Data Extraction Error" ]; then
            echo "- **Check Bridge Status**: Verify WhatsApp bridge is running in Codespace"
            echo "- **Verify Date**: Ensure $TARGET_DATE has actual production reports"
            echo "- **Database Access**: Check Codespace connectivity and SQLite database"
            echo "- **Retry Command**: Try \`/pdr-cloud $TARGET_DATE\` again"
          elif [ "$FAILURE_CATEGORY" = "Processing Timeout" ]; then
            echo "- **Check Claude Processing**: Look for @claude response in this issue"
            echo "- **GitHub Actions**: Verify Claude Code Action permissions"
            echo "- **Manual Processing**: Use \`/pdr-single\` as fallback"
            echo "- **PR Creation**: Check if PR was created but not detected"
          elif [ "$FAILURE_CATEGORY" = "Issue Creation Error" ]; then
            echo "- **GitHub Permissions**: Verify PAT_WITH_CODESPACE token scope"
            echo "- **Repository Access**: Check workflow permissions"
            echo "- **API Limits**: Verify GitHub API quota availability"
          else
            echo "- **Workflow Configuration**: Check .github/workflows/cloud-pdr-processing.yml"
            echo "- **GitHub Actions**: Verify repository dispatch configuration"
            echo "- **Token Permissions**: Check PAT_WITH_CODESPACE scopes"
          fi)
          
          ## ‚ö° Recovery Options
          1. **Retry Automated**: Use \`/pdr-cloud $TARGET_DATE\` command again
          2. **Manual Processing**: Use \`/pdr-single\` with WhatsApp data
          3. **Debug Mode**: Check Codespace and bridge health manually
          4. **Escalate**: Tag @karstegg for investigation
          
          **Issue Status**: ‚è≥ Left open for manual resolution or retry"
          
          echo "‚úÖ Comprehensive failure report added to issue #$ISSUE_NUMBER"
        else
          echo "‚ùå Processing failed before issue creation - check workflow configuration"
          
          # Create emergency notification issue
          gh issue create \
            --title "üö® Emergency: Cloud Processing Failure - $TARGET_DATE" \
            --body "**Critical Workflow Failure**
          
          **Date**: $TARGET_DATE
          **Category**: $FAILURE_CATEGORY  
          **Details**: $FAILURE_DETAILS
          **Workflow**: [View Logs](https://github.com/karstegg/MarthaVault/actions/runs/$GITHUB_RUN_ID)
          
          **Impact**: Autonomous processing pipeline completely failed before issue creation.
          
          **Action Required**: 
          - Verify workflow configuration
          - Check GitHub Actions permissions  
          - Test \`/pdr-cloud\` command manually
          
          @karstegg" \
            --label "emergency" \
            --label "pdr-cloud" || echo "Failed to create emergency issue"
        fi