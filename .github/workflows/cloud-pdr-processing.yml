name: Autonomous Cloud PDR Processing

on:
  repository_dispatch:
    types: [pdr-cloud]

jobs:
  autonomous-pdr-processing:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
      pull-requests: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Check Bridge Health & Extract WhatsApp Messages
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: extract
      run: |
        CODESPACE_NAME="cuddly-guacamole-496vp6p46wg39r"
        TARGET_DATE="${{ github.event.client_payload.date }}"
        
        echo "ğŸ” Checking WhatsApp bridge health and extracting messages for $TARGET_DATE..."
        
        # Check if bridge service is running and restart if needed
        echo "ğŸ”§ Checking bridge service status..."
        BRIDGE_STATUS=$(gh codespace ssh -c $CODESPACE_NAME -- \
          "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh status" 2>/dev/null || echo "Bridge not running")
        
        if [[ "$BRIDGE_STATUS" == *"not running"* ]]; then
          echo "ğŸš¨ Bridge not running - starting service..."
          gh codespace ssh -c $CODESPACE_NAME -- \
            "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh start"
          sleep 10  # Give bridge time to start
          echo "âœ… Bridge service started"
        else
          echo "âœ… Bridge service is running"
        fi
        
        # Extract messages from Codespace SQLite database
        echo "ğŸ“‹ Extracting messages with production keywords..."
        gh codespace ssh -c $CODESPACE_NAME -- \
          "sqlite3 /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db \"SELECT timestamp, chat_jid, sender, content FROM messages WHERE timestamp BETWEEN '${TARGET_DATE} 00:00:00+00:00' AND '${TARGET_DATE} 23:59:59+00:00' AND chat_jid = '27834418149-1537194373@g.us' AND (content LIKE '%ROM%' OR content LIKE '%Safety%' OR content LIKE '%Gloria Report%' OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%' OR content LIKE '%Decline%' OR content LIKE '%Product%' OR content LIKE '%Loads%') ORDER BY timestamp;\"" > extracted_messages.txt
        
        # Debug: Check if file was created and has content
        if [ -f extracted_messages.txt ]; then
          echo "âœ… Extraction file created"
          echo "ğŸ“„ File size: $(stat -c%s extracted_messages.txt 2>/dev/null || stat -f%z extracted_messages.txt 2>/dev/null || echo 'unknown') bytes"
        else
          echo "âŒ Extraction file not created"
        fi
           
        # Count messages and validate extraction
        MESSAGE_COUNT=$(wc -l < extracted_messages.txt)
        echo "ğŸ“Š Found $MESSAGE_COUNT WhatsApp messages for $TARGET_DATE"
        
        # Early exit if no data found
        if [ "$MESSAGE_COUNT" -eq 0 ]; then
          echo "âš ï¸  No production messages found for $TARGET_DATE"
          echo "ğŸ›‘ STOPPING PROCESSING - No data available for this date"
          echo "Possible reasons:"
          echo "  - Date may be a weekend/holiday with no reports"
          echo "  - Bridge may have been offline during report time"
          echo "  - Reports may use different keywords not in search filter"
          echo "  - Date format may be incorrect (use YYYY-MM-DD)"
          
          # Create a no-data notification issue and exit
          gh issue create \
            --title "âŒ No Data Found: $TARGET_DATE" \
            --body "**No production data available for $TARGET_DATE**

          ğŸ“Š **Search Results**: $MESSAGE_COUNT messages found
          ğŸ” **Search Criteria**: ROM, Production, Gloria, Nchwaning, S&W keywords
          â° **Time Range**: Full day (00:00-23:59 UTC)
          
          **Possible Reasons**:
          - Weekend/holiday - no production reports sent
          - WhatsApp bridge offline during report time (4:00-6:00 AM SAST)
          - Engineers used different messaging keywords
          - Date format incorrect or date does not exist
          
          **Bridge Status**: Checked and restarted if needed
          **Recommendation**: Verify date and retry, or process manually if reports exist" \
            --label "no-data" \
            --label "pdr-cloud"
            
          exit 0  # Exit successfully but stop processing
        fi
        
        # Output variables for next steps
        echo "message_count=$MESSAGE_COUNT" >> $GITHUB_OUTPUT
        echo "target_date=$TARGET_DATE" >> $GITHUB_OUTPUT
        
        # Preview extracted data (first 3 lines)
        echo "ğŸ“‹ Message preview:"
        head -3 extracted_messages.txt || echo "No data to preview"
        
    - name: Create Processing Issue
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: issue
      if: steps.extract.outputs.message_count > 0
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        echo "ğŸ“ Creating processing issue for autonomous Claude processing..."
        
        # Debug: Show what data we're about to include
        echo "ğŸ” Data to include in issue (first 5 lines):"
        head -5 extracted_messages.txt || echo "No data to preview"
        echo "ğŸ“„ Full data file content:"
        cat extracted_messages.txt | wc -l
        
        # Create issue with extracted data and @claude mention
        ISSUE_URL=$(gh issue create \
          --title "ğŸ¤– Autonomous PDR Processing - $TARGET_DATE" \
          --label "autonomous-processing" \
          --label "pdr-cloud" \
          --body "$(cat <<EOF
        @claude Please process these daily production reports autonomously:

        **Processing Request Details:**
        - **Date**: $TARGET_DATE
        - **Messages Found**: $MESSAGE_COUNT WhatsApp messages
        - **Processing Mode**: Fully Autonomous
        - **Triggered From**: /pdr-cloud command in local Claude Code

        **Processing Instructions:**
        1. Follow /pdr-single workflow logic exactly
        2. Use appropriate Report Templates:
           - Standard Mine Site Report Template â†’ Gloria, Nchwaning 2, Nchwaning 3  
           - Shafts & Winders Report Template â†’ Infrastructure reports
        3. Create both JSON and Markdown files for each detected site
        4. Use exact folder structure: \`daily_production/data/YYYY-MM/DD/\`
        5. Include proper engineer assignment based on site
        6. Add source validation sections in JSON files
        7. **CRITICAL: Create Pull Request immediately** after generating files - do not provide links, actually create the PR
        8. **Use this exact PR creation command after committing files**:
           \`gh pr create --title \"feat: Autonomous PDR processing for $TARGET_DATE\" --body \"Auto-processed daily production reports\" --head [your-branch] --base master\`
        9. **This issue will auto-close** after successful processing

        **Expected Sites**: Gloria, Nchwaning 2, Nchwaning 3, Shafts & Winders

        **Raw WhatsApp Data:**
        \`\`\`
        $(cat extracted_messages.txt)
        \`\`\`

        **Auto-Processing Request**: Please process and create PR with results immediately. The workflow will handle auto-merge and issue closure.
        EOF
        )")
        
        # Extract issue number from URL
        ISSUE_NUMBER=$(echo $ISSUE_URL | grep -o '[0-9]*$')
        echo "âœ… Created processing issue #$ISSUE_NUMBER"
        echo "ğŸ”— Issue URL: $ISSUE_URL"
        
        # Output variables for monitoring steps
        echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
        echo "issue_url=$ISSUE_URL" >> $GITHUB_OUTPUT
        
    - name: Monitor Claude Processing
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: monitor
      if: steps.extract.outputs.message_count > 0
      run: |
        ISSUE_NUMBER="${{ steps.issue.outputs.issue_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        
        echo "ğŸ‘€ Monitoring Claude processing for issue #$ISSUE_NUMBER"
        echo "â³ Waiting for Claude to process reports and create PR..."
        
        # Monitor for PR creation (max 30 minutes)
        TIMEOUT=120  # 120 iterations * 15 seconds = 30 minutes
        PR_FOUND=false
        
        for i in $(seq 1 $TIMEOUT); do
          sleep 15
          
          # Check if Claude has created a PR for this specific date
          # Look for PRs created recently (within last 2 hours) with specific patterns
          PR_LIST=$(gh pr list --state open --json number,title,createdAt --jq '
            map(select(
              (.title | contains("Autonomous PDR processing for '$TARGET_DATE'")) or
              (.title | contains("feat: Autonomous PDR processing for '$TARGET_DATE'")) or
              (.title | contains("PDR processing for '$TARGET_DATE'")) or
              (.title | contains("'$TARGET_DATE'") and (.title | contains("PDR") or .title | contains("production")))
            ))
          ')
          
          PR_COUNT=$(echo "$PR_LIST" | jq length)
          
          if [ "$PR_COUNT" -gt 0 ]; then
            PR_NUMBER=$(echo "$PR_LIST" | jq -r '.[0].number')
            PR_TITLE=$(echo "$PR_LIST" | jq -r '.[0].title')
            echo "âœ… Claude created PR #$PR_NUMBER: $PR_TITLE"
            echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
            PR_FOUND=true
            break
          fi
          
          # Show progress every minute
          if [ $((i % 4)) -eq 0 ]; then
            echo "â³ Still waiting for Claude processing... ($((i*15))s elapsed)"
          fi
        done
        
        if [ "$PR_FOUND" = false ]; then
          echo "âŒ Timeout: No PR created within 30 minutes"
          echo "ğŸ” Check issue #$ISSUE_NUMBER for Claude's progress"
          exit 1
        fi
        
    - name: Auto-Validate and Auto-Merge PR
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: merge
      if: steps.extract.outputs.message_count > 0
      run: |
        PR_NUMBER="${{ steps.monitor.outputs.pr_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        echo "ğŸ” Auto-validating PR #$PR_NUMBER for merge..."
        
        # Checkout the PR to inspect files
        gh pr checkout $PR_NUMBER
        
        # Calculate expected directory structure
        YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
        DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
        EXPECTED_DIR="daily_production/data/${YEAR_MONTH}/${DAY}"
        
        echo "ğŸ“ Checking for files in: $EXPECTED_DIR"
        
        # Validation checks
        VALIDATION_PASSED=true
        VALIDATION_MESSAGES=""
        
        # Check 1: Expected directory exists
        if [ -d "$EXPECTED_DIR" ]; then
          echo "âœ… Directory structure correct: $EXPECTED_DIR"
        else
          echo "âŒ Expected directory not found: $EXPECTED_DIR"
          VALIDATION_PASSED=false
          VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- Missing directory: $EXPECTED_DIR"
        fi
        
        # Check 2: Count files (JSON and Markdown)
        if [ -d "$EXPECTED_DIR" ]; then
          JSON_COUNT=$(find "$EXPECTED_DIR" -name "*.json" | wc -l)
          MD_COUNT=$(find "$EXPECTED_DIR" -name "*.md" | wc -l)
          TOTAL_FILES=$((JSON_COUNT + MD_COUNT))
          
          echo "ğŸ“Š Found $JSON_COUNT JSON files and $MD_COUNT Markdown files"
          
          if [ "$TOTAL_FILES" -ge 2 ]; then
            echo "âœ… Sufficient files created: $TOTAL_FILES files"
          else
            echo "âŒ Insufficient files: Only $TOTAL_FILES files found (minimum 2 required)"
            VALIDATION_PASSED=false
            VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- Only $TOTAL_FILES files found (need at least 2)"
          fi
        else
          TOTAL_FILES=0
        fi
        
        # Check 3: File size validation (not empty, reasonable size)
        if [ "$TOTAL_FILES" -gt 0 ]; then
          EMPTY_FILES=$(find "$EXPECTED_DIR" \( -name "*.json" -o -name "*.md" \) -empty | wc -l)
          if [ "$EMPTY_FILES" -eq 0 ]; then
            echo "âœ… No empty files detected"
          else
            echo "âŒ Found $EMPTY_FILES empty files"
            VALIDATION_PASSED=false
            VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- $EMPTY_FILES empty files detected"
          fi
        fi
        
        # Proceed with merge if validation passed
        if [ "$VALIDATION_PASSED" = true ]; then
          echo "ğŸ¯ All validations passed - proceeding with auto-merge"
          
          # Auto-merge PR with descriptive commit message
          gh pr merge $PR_NUMBER --squash --delete-branch \
            --subject "feat: Autonomous daily production reports - $TARGET_DATE" \
            --body "Auto-processed and auto-merged production reports for $TARGET_DATE

        ğŸ“Š **Processing Summary:**
        - Extracted $MESSAGE_COUNT WhatsApp messages from Codespace
        - Generated $TOTAL_FILES report files ($JSON_COUNT JSON, $MD_COUNT Markdown)
        - Applied Report Templates for standardized formatting
        - Validated file structure and content automatically
        - Completed autonomous processing via /pdr-cloud command

        ğŸ“ **Files Location:** \`daily_production/data/$YEAR_MONTH/$DAY/\`

        ğŸ¤– **Fully autonomous processing** - no manual intervention required
        âœ… **Auto-validation passed** - all quality checks successful"
          
          echo "âœ… PR #$PR_NUMBER auto-merged successfully"
          echo "files_created=$TOTAL_FILES" >> $GITHUB_OUTPUT
          
        else
          echo "âŒ Validation failed - PR will not be auto-merged"
          echo "ğŸ” Validation issues:$VALIDATION_MESSAGES"
          echo "ğŸ“‹ Manual review required for PR #$PR_NUMBER"
          exit 1
        fi
        
    - name: Auto-Close Issue with Success Summary  
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: success() && steps.extract.outputs.message_count > 0
      run: |
        ISSUE_NUMBER="${{ steps.issue.outputs.issue_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        FILES_CREATED="${{ steps.merge.outputs.files_created }}"
        PR_NUMBER="${{ steps.monitor.outputs.pr_number }}"
        
        echo "ğŸ¯ Auto-closing issue #$ISSUE_NUMBER with success summary"
        
        # Close issue with comprehensive summary
        gh issue close $ISSUE_NUMBER \
          --comment "âœ… **Autonomous Processing Successfully Completed**

        ## ğŸ“Š Processing Summary
        - **Date Processed**: $TARGET_DATE
        - **WhatsApp Messages**: $MESSAGE_COUNT messages extracted
        - **Files Generated**: $FILES_CREATED files (JSON + Markdown)
        - **PR Created**: #$PR_NUMBER (auto-merged)
        - **Processing Time**: Complete workflow in under 10 minutes

        ## ğŸ¯ Workflow Steps Completed
        - âœ… WhatsApp data extracted from Codespace SQLite database
        - âœ… Claude processed reports using /pdr-single workflow logic  
        - âœ… Applied appropriate Report Templates for each site
        - âœ… Generated structured JSON and readable Markdown files
        - âœ… Validated file structure and content automatically
        - âœ… Auto-merged PR after successful validation
        - âœ… Auto-closed issue with this summary

        ## ğŸ“ Results Available
        **Location**: \`daily_production/data/$(echo $TARGET_DATE | cut -d'-' -f1,2)/$(echo $TARGET_DATE | cut -d'-' -f3)/\`
        **Access**: Run \`git pull\` to sync files locally

        ## ğŸš€ Automation Details
        **Triggered**: Via \`/pdr-cloud $TARGET_DATE\` command from local Claude Code
        **Processing**: Fully autonomous - no manual intervention required
        **Quality**: Auto-validated against established templates and structure

        ---
        ğŸ¤– **Issue auto-closed** - autonomous processing pipeline complete"
        
        echo "âœ… Issue #$ISSUE_NUMBER closed with success summary"
        echo "ğŸ‰ Autonomous processing pipeline completed successfully!"
        
    - name: Auto-Push to Local Repository
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: success() && steps.extract.outputs.message_count > 0
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        FILES_CREATED="${{ steps.merge.outputs.files_created }}"
        
        echo "ğŸ”„ Initiating automatic push to local repository..."
        
        # Send repository dispatch to trigger local sync
        echo "ğŸ“¡ Sending auto-sync dispatch to local repository..."
        echo '{"event_type": "auto-sync", "client_payload": {"date": "'$TARGET_DATE'", "files_created": "'$FILES_CREATED'", "trigger": "cloud-processing-complete"}}' | gh api repos/karstegg/MarthaVault/dispatches --method POST --input -
        
        if [ $? -eq 0 ]; then
          echo "âœ… Auto-sync dispatch sent successfully"
          echo "ğŸ”„ Local repository will automatically update with new files"
          
          # Create notification issue for confirmation
          gh issue create \
            --title "ğŸ”„ Auto-Push Complete: $TARGET_DATE" \
            --body "âœ… **Automatic Push to Local Repository Initiated**

          ## ğŸ“Š Processing Complete
          - **Date**: $TARGET_DATE
          - **Files Created**: $FILES_CREATED files
          - **Status**: âœ… Auto-merged and pushed to local

          ## ğŸš€ Auto-Push Details
          - **Trigger**: Repository dispatch sent to local environment
          - **Action**: New files automatically pushed to local repository
          - **Safety**: Only new/changed files affected, existing local files untouched

          ## ğŸ“ Files Now Available Locally
          Location: \`daily_production/data/$(echo $TARGET_DATE | cut -d'-' -f1,2)/$(echo $TARGET_DATE | cut -d'-' -f3)/\`
          
          **Files**:
          - JSON database files for analysis
          - Markdown reports for review
          - All properly organized in daily_production structure

          ---
          ğŸ¤– **Fully Autonomous** - Files automatically available in your local repository" \
            --label "auto-push" \
            --label "completed"
          
          echo "ğŸ“§ Auto-push confirmation notification created"
        else
          echo "âŒ Failed to send auto-sync dispatch"
          echo "ğŸ” Check repository dispatch configuration"
        fi
        
    - name: Handle Processing Failures
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: failure()
      run: |
        ISSUE_NUMBER="${{ steps.issue.outputs.issue_number }}"
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        
        if [ -n "$ISSUE_NUMBER" ]; then
          echo "âŒ Processing failed - updating issue #$ISSUE_NUMBER with error details"
          
          gh issue comment $ISSUE_NUMBER \
            --body "âŒ **Autonomous Processing Failed**

          **Date**: $TARGET_DATE  
          **Status**: Workflow encountered errors during processing
          
          **Possible Issues**:
          - Claude processing timeout (>15 minutes)
          - File validation failures
          - Codespace connectivity issues
          - GitHub API errors
          
          **Next Steps**:
          - Check workflow logs for specific error details
          - Verify Codespace is active and accessible
          - Retry processing manually if needed
          - Issue left open for manual resolution
          
          **Debug**: Check GitHub Actions logs for this workflow run"
          
          echo "ğŸ“ Added failure comment to issue #$ISSUE_NUMBER"
          echo "ğŸ” Issue left open for manual troubleshooting"
        else
          echo "âŒ Processing failed before issue creation"
          echo "ğŸ” Check workflow logs for extraction or setup errors"
        fi