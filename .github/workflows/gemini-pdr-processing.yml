name: Gemini AI PDR Processing

on:
  repository_dispatch:
    types: [pdr-gemini]

jobs:
  gemini-pdr-processing:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
      pull-requests: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Check Bridge Health & Extract WhatsApp Messages
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: extract
      run: |
        CODESPACE_NAME="cuddly-guacamole-496vp6p46wg39r"
        TARGET_DATE="${{ github.event.client_payload.date }}"
        ADDITIONAL_INSTRUCTIONS="${{ github.event.client_payload.instructions }}"
        
        echo "üîç Checking WhatsApp bridge health and extracting messages for $TARGET_DATE..."
        
        # Check if bridge service is running and restart if needed
        echo "üîß Checking bridge service status..."
        BRIDGE_STATUS=$(gh codespace ssh -c $CODESPACE_NAME -- \
          "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh status" 2>/dev/null || echo "Bridge not running")
        
        if [[ "$BRIDGE_STATUS" == *"not running"* ]]; then
          echo "üö® Bridge not running - starting service..."
          gh codespace ssh -c $CODESPACE_NAME -- \
            "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh start"
          sleep 10  # Give bridge time to start
          echo "‚úÖ Bridge service started"
        else
          echo "‚úÖ Bridge service is running"
        fi
        
        # Extract messages from Codespace SQLite database
        echo "üìã Extracting messages with production keywords..."
        gh codespace ssh -c $CODESPACE_NAME -- \
          "sqlite3 /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db \"SELECT timestamp, chat_jid, sender, content FROM messages WHERE timestamp BETWEEN '${TARGET_DATE} 00:00:00+00:00' AND '${TARGET_DATE} 23:59:59+00:00' AND chat_jid = '27834418149-1537194373@g.us' AND (content LIKE '%ROM%' OR content LIKE '%Safety%' OR content LIKE '%Gloria Report%' OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%' OR content LIKE '%Decline%' OR content LIKE '%Product%' OR content LIKE '%Loads%') ORDER BY timestamp;\"" > extracted_messages.txt
        
        # Debug: Check if file was created and has content
        if [ -f extracted_messages.txt ]; then
          echo "‚úÖ Extraction file created"
          echo "üìÑ File size: $(stat -c%s extracted_messages.txt 2>/dev/null || stat -f%z extracted_messages.txt 2>/dev/null || echo 'unknown') bytes"
        else
          echo "‚ùå Extraction file not created"
        fi
           
        # Count messages and validate extraction
        MESSAGE_COUNT=$(wc -l < extracted_messages.txt)
        echo "üìä Found $MESSAGE_COUNT WhatsApp messages for $TARGET_DATE"
        
        # Log if no data found (but don't exit - let conditional handle it)
        if [ "$MESSAGE_COUNT" -eq 0 ]; then
          echo "‚ö†Ô∏è  No production messages found for $TARGET_DATE"
          echo "Data may not be available or date may be incorrect"
        fi
        
        # Output variables for next steps
        echo "message_count=$MESSAGE_COUNT" >> $GITHUB_OUTPUT
        echo "target_date=$TARGET_DATE" >> $GITHUB_OUTPUT
        
        # CRITICAL FIX: Save extracted data for Gemini processing (GitHub Actions has multiline output limits)
        if [ "$MESSAGE_COUNT" -gt 0 ]; then
          # Copy data to workspace for Gemini access
          cp extracted_messages.txt whatsapp_data_for_gemini.txt
          echo "data_file_created=true" >> $GITHUB_OUTPUT
        else
          echo "data_file_created=false" >> $GITHUB_OUTPUT
        fi
        
        # Preview extracted data (first 3 lines)
        echo "üìã Message preview:"
        head -3 extracted_messages.txt || echo "No data to preview"
    
    - name: Handle No Data Found
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: steps.extract.outputs.message_count == '0'
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        echo "üõë STOPPING PROCESSING - No data available for $TARGET_DATE"
        echo "Possible reasons:"
        echo "  - Date may be a weekend/holiday with no reports"
        echo "  - Bridge may have been offline during report time"
        echo "  - Reports may use different keywords not in search filter"
        echo "  - Date format may be incorrect (use YYYY-MM-DD)"
        
        # Create a no-data notification issue
        gh issue create \
          --title "‚ùå No Data Found (Gemini): $TARGET_DATE" \
          --body "**No production data available for $TARGET_DATE**

        üìä **Search Results**: 0 messages found
        üîç **Search Criteria**: ROM, Production, Gloria, Nchwaning, S&W keywords
        ‚è∞ **Time Range**: Full day (00:00-23:59 UTC)
        ü§ñ **Processing**: Gemini AI CLI action workflow
        
        **Possible Reasons**:
        - Weekend/holiday - no production reports sent
        - WhatsApp bridge offline during report time (4:00-6:00 AM SAST)
        - Engineers used different messaging keywords
        - Date format incorrect or date does not exist
        
        **Bridge Status**: Checked and restarted if needed
        **Recommendation**: Verify date and retry, or process manually if reports exist" \
          --label "no-data" \
          --label "pdr-gemini"
        
        echo "‚úÖ Created no-data notification issue"
        
    - name: Install Gemini CLI
      if: steps.extract.outputs.message_count > 0
      run: |
        echo "üì¶ Installing Gemini CLI..."
        npm install --silent --no-audit --prefer-offline --global @google/gemini-cli@latest
        
        echo "üîç Verifying installation..."
        if command -v gemini >/dev/null 2>&1; then
          gemini --version || echo "Gemini CLI installed successfully (version command not available)"
        else
          echo "Error: Gemini CLI not found in PATH"
          exit 1
        fi
        
    - name: Read WhatsApp Data and Process with Gemini AI
      id: gemini_process
      if: steps.extract.outputs.message_count > 0
      run: |
        TARGET_DATE="${{ github.event.client_payload.date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        echo "üìã Reading WhatsApp data for Gemini processing..."
        WHATSAPP_DATA=$(cat whatsapp_data_for_gemini.txt)
        
        echo "ü§ñ Processing with Gemini AI..."
        
        # Create the full prompt with embedded data
        cat > gemini_prompt.txt << 'EOF'
        You are processing daily production reports for Assmang's Black Rock mining operations.
        
        **TASK**: Process the WhatsApp production data below and output structured file content for each detected mine site.
        
        **SITES TO PROCESS**:
        - Gloria Mine (Engineer: Sipho Dubazane)
        - Nchwaning 2 (Engineer: Sikelela Nzuza) 
        - Nchwaning 3 (Engineer: Sello Sease)
        - Shafts & Winders (Engineer: Xavier Peterson)
        
        **REQUIREMENTS**:
        1. Analyze data for each site and output both JSON and Markdown content
        2. Follow the exact JSON schema and Markdown template provided in GEMINI.md
        3. NEVER INVENT DATA - extract only actual values from source
        4. Include source_validation section for all extracted data
        5. Use null for missing data, do not fabricate
        6. Use target date: ${TARGET_DATE} for all file content
        
        **OUTPUT FORMAT**: For each site detected, output in this exact format:
        ```
        === SITE: [site_name] ===
        === JSON_START ===
        [complete JSON content here]
        === JSON_END ===
        === MARKDOWN_START ===
        [complete Markdown content here]
        === MARKDOWN_END ===
        ```
        
        **CRITICAL**: Follow GEMINI.md configuration exactly. Every data point must be traceable to source.
        
        **WhatsApp Data to Process**:
        ```
        EOF
        
        # Append the actual data
        cat whatsapp_data_for_gemini.txt >> gemini_prompt.txt
        
        # Complete the prompt
        cat >> gemini_prompt.txt << 'EOF'
        ```
        
        **IMPORTANT**: You cannot create files directly. Instead, output the complete file content in the format specified above.
        The workflow will parse your output and create the actual files.
        
        **TASK BREAKDOWN**:
        1. Analyze the WhatsApp data and identify mine sites (Gloria, Nchwaning 2, Nchwaning 3, Shafts & Winders)
        2. For each site found in the data:
           a. Generate complete JSON content following GEMINI.md schema
           b. Generate complete Markdown content following GEMINI.md template
           c. Use exact site identification and engineer assignment
           d. Include comprehensive source validation sections
        3. Output everything in the specified delimited format for parsing
        EOF
        
        # Process with Gemini and capture output
        export GEMINI_API_KEY="${{ secrets.GEMINI_API_KEY }}"
        echo "ü§ñ Running Gemini AI processing..."
        gemini --prompt "$(cat gemini_prompt.txt)" > gemini_output.txt 2>&1
        
        echo "üìÑ Gemini processing completed - checking output..."
        
        # Check if output contains expected format
        if grep -q "=== SITE:" gemini_output.txt; then
          echo "‚úÖ Gemini generated structured output"
          echo "üìã Preview of Gemini output:"
          head -20 gemini_output.txt
        else
          echo "‚ùå Gemini output format issue - no site delimiters found"
          echo "üìÑ Raw Gemini output:"
          cat gemini_output.txt
          exit 1
        fi
    
    - name: Parse Gemini Output and Create Files
      id: parse_output
      if: steps.gemini_process.conclusion == 'success'
      run: |
        TARGET_DATE="${{ github.event.client_payload.date }}"
        YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
        DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
        TARGET_DIR="daily_production/data/${YEAR_MONTH}/${DAY}"
        
        echo "üìÇ Creating target directory: $TARGET_DIR"
        mkdir -p "$TARGET_DIR"
        
        echo "üîç Parsing Gemini output to create files..."
        
        # Parse the structured output and create files
        python3 << 'PYTHON_EOF'
        import re
        import json
        import os
        
        target_date = os.environ['TARGET_DATE']
        target_dir = os.environ['TARGET_DIR']
        
        # Read Gemini output
        with open('gemini_output.txt', 'r') as f:
            content = f.read()
        
        # Extract site sections
        site_pattern = r'=== SITE: ([^=]+) ===\s*=== JSON_START ===\s*(.*?)\s*=== JSON_END ===\s*=== MARKDOWN_START ===\s*(.*?)\s*=== MARKDOWN_END ==='
        sites = re.findall(site_pattern, content, re.DOTALL)
        
        files_created = 0
        
        for site_name, json_content, markdown_content in sites:
            site_name = site_name.strip()
            site_slug = site_name.lower().replace(' ', '_').replace('&', '').replace('  ', '_')
            
            # Map site names to standard slugs
            if 'gloria' in site_slug:
                site_slug = 'gloria'
            elif 'nchwaning_2' in site_slug or 'nchwaning2' in site_slug:
                site_slug = 'nchwaning2'
            elif 'nchwaning_3' in site_slug or 'nchwaning3' in site_slug:
                site_slug = 'nchwaning3'
            elif 'shafts' in site_slug and 'winders' in site_slug:
                site_slug = 'shafts_winders'
            
            print(f"üìÑ Processing site: {site_name} -> {site_slug}")
            
            # Create JSON file
            json_filename = f"{target_date}_{site_slug}.json"
            json_path = os.path.join(target_dir, json_filename)
            
            try:
                # Validate JSON
                json.loads(json_content.strip())
                with open(json_path, 'w') as f:
                    f.write(json_content.strip())
                print(f"‚úÖ Created JSON: {json_path}")
                files_created += 1
            except json.JSONDecodeError as e:
                print(f"‚ùå Invalid JSON for {site_name}: {e}")
                continue
            
            # Create Markdown file
            md_filename = f"{target_date} ‚Äì {site_name} Daily Report.md"
            md_path = os.path.join(target_dir, md_filename)
            
            with open(md_path, 'w') as f:
                f.write(markdown_content.strip())
            print(f"‚úÖ Created Markdown: {md_path}")
            files_created += 1
        
        print(f"üìä Total files created: {files_created}")
        
        # Output for workflow
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"files_created={files_created}\n")
            f.write(f"sites_processed={len(sites)}\n")
        
        PYTHON_EOF
        
        echo "‚úÖ File parsing and creation completed"
    
    - name: Verify File Creation
      id: verify_files
      if: steps.parse_output.conclusion == 'success'
      run: |
        TARGET_DATE="${{ github.event.client_payload.date }}"
        YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
        DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
        TARGET_DIR="daily_production/data/${YEAR_MONTH}/${DAY}"
        FILES_CREATED="${{ steps.parse_output.outputs.files_created }}"
        SITES_PROCESSED="${{ steps.parse_output.outputs.sites_processed }}"
        
        echo "üîç Verifying files were created by parsing step in: $TARGET_DIR"
        echo "üìä Parser reported: $FILES_CREATED files created from $SITES_PROCESSED sites"
        
        # Count actual files created
        JSON_FILES=$(find "$TARGET_DIR" -name "*.json" 2>/dev/null | wc -l || echo "0")
        MD_FILES=$(find "$TARGET_DIR" -name "*.md" 2>/dev/null | wc -l || echo "0")
        TOTAL_FILES=$((JSON_FILES + MD_FILES))
        
        echo "üìä Verification: $JSON_FILES JSON, $MD_FILES Markdown = $TOTAL_FILES total files"
        
        # List created files
        if [ -d "$TARGET_DIR" ]; then
          echo "üìÇ Files in $TARGET_DIR:"
          ls -la "$TARGET_DIR" || echo "Directory exists but is empty"
        else
          echo "‚ö†Ô∏è Target directory does not exist"
        fi
        
        # Validation check
        if [ "$TOTAL_FILES" -eq "$FILES_CREATED" ] && [ "$TOTAL_FILES" -gt 0 ]; then
          echo "‚úÖ File count verification passed"
        else
          echo "‚ùå File count mismatch: Expected $FILES_CREATED, Found $TOTAL_FILES"
        fi
        
        echo "json_files_count=$JSON_FILES" >> $GITHUB_OUTPUT
        echo "md_files_count=$MD_FILES" >> $GITHUB_OUTPUT
        echo "total_files=$TOTAL_FILES" >> $GITHUB_OUTPUT
        echo "target_dir=$TARGET_DIR" >> $GITHUB_OUTPUT
        
    - name: Validate Generated Files
      id: validate
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        TARGET_DIR="${{ steps.verify_files.outputs.target_dir }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        echo "üîç Validating generated files..."
        
        # Validation checks
        VALIDATION_PASSED=true
        VALIDATION_MESSAGES=""
        
        # Check 1: Target directory exists
        if [ -d "$TARGET_DIR" ]; then
          echo "‚úÖ Directory structure correct: $TARGET_DIR"
        else
          echo "‚ùå Target directory not found: $TARGET_DIR"
          VALIDATION_PASSED=false
          VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- Missing directory: $TARGET_DIR"
        fi
        
        # Check 2: Count generated files
        JSON_COUNT="${{ steps.verify_files.outputs.json_files_count }}"
        MD_COUNT="${{ steps.verify_files.outputs.md_files_count }}"
        TOTAL_FILES="${{ steps.verify_files.outputs.total_files }}"
        
        if [ -d "$TARGET_DIR" ]; then
          
          echo "üìä Found $JSON_COUNT JSON files and $MD_COUNT Markdown files"
          
          # Check for paired files - each JSON should have matching Markdown
          MISSING_PAIRS=""
          for site in gloria nchwaning2 nchwaning3 shafts_winders; do
            JSON_FILE="$TARGET_DIR/${TARGET_DATE}_${site}.json"
            if [ -f "$JSON_FILE" ]; then
              # Check for corresponding markdown file
              MD_EXISTS=false
              for md_file in "$TARGET_DIR"/*Daily\ Report.md "$TARGET_DIR"/*Weekly\ Report.md; do
                if [ -f "$md_file" ] && echo "$md_file" | grep -q -i "$(echo $site | sed 's/shafts_winders/shafts.*winders/g; s/nchwaning/nchwaning/g')"; then
                  MD_EXISTS=true
                  break
                fi
              done
              if [ "$MD_EXISTS" = false ]; then
                MISSING_PAIRS="$MISSING_PAIRS $site"
                echo "‚ùå Missing Markdown report for site: $site (JSON exists)"
              else
                echo "‚úÖ Complete pair for site: $site"
              fi
            fi
          done
          
          if [ -n "$MISSING_PAIRS" ]; then
            echo "‚ùå CRITICAL: Missing Markdown reports for sites:$MISSING_PAIRS"
            VALIDATION_PASSED=false
            VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- Missing Markdown reports for:$MISSING_PAIRS"
            VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- This indicates Gemini AI partial failure"
          fi
          
          # Check 3: File size validation (not empty)
          if [ "$TOTAL_FILES" -gt 0 ]; then
            EMPTY_FILES=$(find "$TARGET_DIR" \( -name "*.json" -o -name "*.md" \) -empty | wc -l)
            if [ "$EMPTY_FILES" -eq 0 ]; then
              echo "‚úÖ No empty files detected"
            else
              echo "‚ùå Found $EMPTY_FILES empty files"
              VALIDATION_PASSED=false
              VALIDATION_MESSAGES="$VALIDATION_MESSAGES\n- $EMPTY_FILES empty files detected"
            fi
          fi
        else
          TOTAL_FILES=0
        fi
        
        # Output validation results
        echo "validation_passed=$VALIDATION_PASSED" >> $GITHUB_OUTPUT
        echo "total_files=$TOTAL_FILES" >> $GITHUB_OUTPUT
        echo "json_count=$JSON_COUNT" >> $GITHUB_OUTPUT
        echo "md_count=$MD_COUNT" >> $GITHUB_OUTPUT
        
        if [ "$VALIDATION_PASSED" = false ]; then
          echo "‚ùå Validation failed. Issues:$VALIDATION_MESSAGES"
        else
          echo "‚úÖ All validations passed"
        fi
        
    - name: Create Pull Request
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: steps.validate.outputs.validation_passed == 'true'
      id: pr
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        TOTAL_FILES="${{ steps.validate.outputs.total_files }}"
        JSON_COUNT="${{ steps.validate.outputs.json_count }}"
        MD_COUNT="${{ steps.validate.outputs.md_count }}"
        YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
        DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
        
        # Configure git
        git config --global user.email "noreply@google.com"
        git config --global user.name "Gemini AI"
        
        # Create feature branch
        BRANCH_NAME="gemini/pdr-processing-$TARGET_DATE"
        git checkout -b "$BRANCH_NAME"
        
        # Add generated files
        git add daily_production/data/
        
        # Commit changes
        git commit -m "feat: Gemini AI daily production reports - $TARGET_DATE

        ü§ñ Auto-processed production reports via Gemini AI CLI action
        üìä Processed $MESSAGE_COUNT WhatsApp messages
        üìÅ Generated $TOTAL_FILES files ($JSON_COUNT JSON, $MD_COUNT Markdown)
        üìç Location: daily_production/data/$YEAR_MONTH/$DAY/

        ‚ú® Generated with Gemini AI
        Co-Authored-By: Gemini AI <noreply@google.com>"
        
        # Push branch
        git push origin "$BRANCH_NAME"
        
        # Create PR
        PR_URL=$(gh pr create \
          --title "feat: Gemini AI PDR processing - $TARGET_DATE" \
          --body "$(cat <<EOF
        ## ü§ñ Gemini AI Daily Production Report Processing

        **Date Processed**: $TARGET_DATE  
        **Processing Method**: Free Gemini AI CLI action  
        **Messages Processed**: $MESSAGE_COUNT WhatsApp messages  
        **Files Generated**: $TOTAL_FILES files ($JSON_COUNT JSON + $MD_COUNT Markdown)  

        ### üìä Processing Summary
        - ‚úÖ WhatsApp data extracted from Codespace SQLite database
        - ‚úÖ Processed via Gemini 2.0 Flash Exp model (FREE)
        - ‚úÖ Applied PDR templates and validation rules
        - ‚úÖ Generated structured JSON and readable Markdown files
        - ‚úÖ Auto-validated file structure and content
        - ‚úÖ Zero cost processing (vs \$0.39/day for Claude)

        ### üìÅ Files Location
        \`daily_production/data/$YEAR_MONTH/$DAY/\`

        ### üéØ Quality Assurance
        - All data extracted from source WhatsApp messages
        - Source validation included in JSON files
        - Professional formatting and analysis
        - Follows established Report Templates

        ### üí∞ Cost Comparison
        - **Gemini AI**: FREE (Google AI Studio generous quotas)
        - **Claude Cloud**: \$0.39/day (\$123/year)
        - **Copilot Agent**: \$10/month subscription

        ---
        ü§ñ **Fully autonomous Gemini AI processing** - Ready for review and merge
        EOF
        )" \
          --head "$BRANCH_NAME" \
          --base master)
        
        # Extract PR number
        PR_NUMBER=$(echo $PR_URL | grep -o '[0-9]*$')
        echo "‚úÖ Created PR #$PR_NUMBER: $PR_URL"
        echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
        echo "pr_url=$PR_URL" >> $GITHUB_OUTPUT
        
    - name: Handle Processing Failures
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: failure()
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        
        echo "‚ùå Gemini AI processing workflow failed"
        
        # Create failure notification issue
        gh issue create \
          --title "‚ùå Gemini AI Processing Failed: $TARGET_DATE" \
          --body "**Gemini AI Processing Workflow Failed**

        **Date**: $TARGET_DATE  
        **Messages Found**: $MESSAGE_COUNT WhatsApp messages  
        **Status**: Workflow encountered errors during processing
        
        **Possible Issues**:
        - Gemini API key missing or invalid
        - Gemini AI model unavailable or rate limited
        - File generation or organization failures
        - Validation errors
        - GitHub API errors
        
        **Next Steps**:
        - Check workflow logs for specific error details
        - Verify GEMINI_API_KEY secret is configured
        - Verify Codespace is active and accessible
        - Try fallback to Claude processing: \`/pdr-cloud $TARGET_DATE\`
        - Issue left open for manual resolution
        
        **Debug**: Check GitHub Actions logs for this workflow run
        **Fallback Available**: Use \`/pdr-cloud\` command if Gemini fails" \
          --label "gemini-failure" \
          --label "pdr-gemini"
        
        echo "üìù Created failure notification issue"
        
    - name: Success Summary
      if: success() && steps.validate.outputs.validation_passed == 'true'
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        MESSAGE_COUNT="${{ steps.extract.outputs.message_count }}"
        TOTAL_FILES="${{ steps.validate.outputs.total_files }}"
        PR_NUMBER="${{ steps.pr.outputs.pr_number }}"
        
        echo "üéâ Gemini AI processing completed successfully!"
        echo ""
        echo "üìä **Processing Summary**:"
        echo "- Date: $TARGET_DATE"
        echo "- Messages: $MESSAGE_COUNT WhatsApp messages"
        echo "- Files: $TOTAL_FILES generated files"
        echo "- PR: #$PR_NUMBER created and ready for review"
        echo "- Cost: FREE (Gemini AI)"
        echo ""
        echo "‚úÖ Ready for manual review and merge"