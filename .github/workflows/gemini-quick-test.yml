name: ğŸ¤– Gemini AI Production PDR Processing
# Force workflow re-registration after cache clearing - 2025-08-18

on:
  workflow_dispatch:
    inputs:
      date:
        description: 'Target date (YYYY-MM-DD)'
        required: true
        default: '2025-07-17'
      instructions:
        description: 'Additional processing instructions (optional)'
        required: false
        default: ''

jobs:
  gemini-pdr-processing:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
      pull-requests: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Check Bridge Health & Extract WhatsApp Messages
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      id: extract
      run: |
        CODESPACE_NAME="cuddly-guacamole-496vp6p46wg39r"
        TARGET_DATE="${{ github.event.client_payload.date || github.event.inputs.date }}"
        
        echo "ğŸ” Checking WhatsApp bridge health and extracting messages for $TARGET_DATE..."
        
        # Check if bridge service is running and restart if needed
        echo "ğŸ”§ Checking bridge service status..."
        BRIDGE_STATUS=$(gh codespace ssh -c $CODESPACE_NAME -- \
          "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh status" 2>/dev/null || echo "Bridge not running")
        
        if [[ "$BRIDGE_STATUS" == *"not running"* ]]; then
          echo "ğŸš¨ Bridge not running - starting service..."
          gh codespace ssh -c $CODESPACE_NAME -- \
            "cd /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge && ./start-bridge-service.sh start"
          sleep 10  # Give bridge time to start
          echo "âœ… Bridge service started"
        else
          echo "âœ… Bridge service is running"
        fi
        
        # Extract messages from Codespace SQLite database
        echo "ğŸ“‹ Extracting messages with production keywords..."
        gh codespace ssh -c $CODESPACE_NAME -- \
          "sqlite3 /workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db \"SELECT timestamp, chat_jid, sender, content FROM messages WHERE timestamp BETWEEN '${TARGET_DATE} 00:00:00+00:00' AND '${TARGET_DATE} 23:59:59+00:00' AND chat_jid = '27834418149-1537194373@g.us' AND (content LIKE '%ROM%' OR content LIKE '%Safety%' OR content LIKE '%Gloria Report%' OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%' OR content LIKE '%Decline%' OR content LIKE '%Product%' OR content LIKE '%Loads%') ORDER BY timestamp;\"" > extracted_messages.txt
        
        # Count messages and validate extraction
        MESSAGE_COUNT=$(wc -l < extracted_messages.txt)
        echo "ğŸ“Š Found $MESSAGE_COUNT WhatsApp messages for $TARGET_DATE"
        
        # Output variables for next steps
        echo "message_count=$MESSAGE_COUNT" >> $GITHUB_OUTPUT
        echo "target_date=$TARGET_DATE" >> $GITHUB_OUTPUT
        
        # Save extracted data for Gemini processing
        if [ "$MESSAGE_COUNT" -gt 0 ]; then
          cp extracted_messages.txt whatsapp_data_for_gemini.txt
          echo "data_file_created=true" >> $GITHUB_OUTPUT
        else
          echo "data_file_created=false" >> $GITHUB_OUTPUT
        fi
        
        # Preview extracted data (first 3 lines)
        echo "ğŸ“‹ Message preview:"
        head -3 extracted_messages.txt || echo "No data to preview"
    
    - name: Handle No Data Found
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      if: steps.extract.outputs.message_count == '0'
      run: |
        TARGET_DATE="${{ steps.extract.outputs.target_date }}"
        echo "ğŸ›‘ STOPPING PROCESSING - No data available for $TARGET_DATE"
        echo "Possible reasons:"
        echo "  - Date may be a weekend/holiday with no reports"
        echo "  - Bridge may have been offline during report time"
        echo "  - Reports may use different keywords not in search filter"
        echo "  - Date format may be incorrect (use YYYY-MM-DD)"
        
        # Create a no-data notification issue
        gh issue create \
          --title "âŒ No Data Found (Gemini Production): $TARGET_DATE" \
          --body "**No production data available for $TARGET_DATE**

        ğŸ“Š **Search Results**: 0 messages found
        ğŸ” **Search Criteria**: ROM, Production, Gloria, Nchwaning, S&W keywords
        â° **Time Range**: Full day (00:00-23:59 UTC)
        ğŸ¤– **Processing**: Gemini AI Production workflow
        
        **Possible Reasons**:
        - Weekend/holiday - no production reports sent
        - WhatsApp bridge offline during report time (4:00-6:00 AM SAST)
        - Engineers used different messaging keywords
        - Date format incorrect or date does not exist
        
        **Bridge Status**: Checked and restarted if needed
        **Recommendation**: Verify date and retry, or process manually if reports exist" \
          --label "no-data" \
          --label "pdr-gemini"
        
        echo "âœ… Created no-data notification issue"
        
    - name: Process with Gemini AI (Production Logic)
      uses: 'google-github-actions/run-gemini-cli@v0.1.11'
      id: gemini_process
      if: steps.extract.outputs.message_count > 0
      env:
        TARGET_DATE: ${{ github.event.client_payload.date || github.event.inputs.date }}
        MESSAGE_COUNT: ${{ steps.extract.outputs.message_count }}
      with:
        gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
        settings: |-
          {
            "model": "gemini-2.5-flash",
            "maxSessionTurns": 15,
            "autoAccept": ["list_directory", "read_file", "write_file", "glob"],
            "telemetry": {"enabled": false}
          }
        prompt: |-
          ## Role
          
          You are a specialized AI assistant for processing daily production reports for Assmang's Black Rock mining operations. You have full access to all available tools and must create comprehensive, professional reports that match established Claude-quality standards.
          
          ## Context
          
          - **Repository**: `${{ github.repository }}`
          - **Processing Date**: `${{ env.TARGET_DATE }}`
          - **Messages Found**: `${{ env.MESSAGE_COUNT }}`
          - **Data Source**: WhatsApp production reports from mine engineers
          - **Output Required**: JSON and Markdown files for each mine site using established templates
          
          ## CRITICAL: Template Compliance Required
          
          **YOU MUST READ AND FOLLOW THE REPORT TEMPLATES**:
          1. **First Action**: Read `Report Templates/Standard Mine Site Report Template.md`
          2. **Second Action**: Read `Report Templates/Shafts & Winders Report Template.md`
          3. **Follow Templates Exactly**: Use the exact structure, formatting, and content organization
          
          **SITES TO PROCESS**:
          - **Gloria Mine** (Engineer: Sipho Dubazane) â†’ Use Standard Mine Site Template
          - **Nchwaning 2** (Engineer: Sikelela Nzuza) â†’ Use Standard Mine Site Template
          - **Nchwaning 3** (Engineer: Sello Sease) â†’ Use Standard Mine Site Template
          - **Shafts & Winders** (Engineer: Xavier Peterson) â†’ Use Shafts & Winders Template
          
          ## MANDATORY: Equipment Code Validation
          
          **ğŸš¨ CRITICAL EQUIPMENT CODES** (Never fabricate these):
          - **RT = Roof Bolter** (NOT Rock Truck - no such equipment exists!)
          - **DT = Dump Truck** (material transport)
          - **FL = Front Loader** (loading equipment)
          - **HD = Hydraulic Drill** (drilling equipment, NOT haul truck)
          - **SR = Scaler** (scaling equipment, NOT service rig)
          - **UV = Utility Vehicle** (support vehicles)
          - **GD = Grader** (NOT GR)
          - **DZ = Dozer** (bulldozer equipment)
          
          **REJECT any reference to "Rock Truck" - use "Roof Bolter" for RT codes**
          
          ## MANDATORY: Date Logic & Operational Context
          
          **Dating Protocol**:
          - **Report Date**: When message received (e.g., 21st August 2025) â†’ File naming date
          - **Data Date**: Previous day's operations (e.g., 20th August 2025) â†’ Production data period
          - **Always use report date for file naming**: `2025-08-21_site.json` even if data is from 20th August
          
          **Operational Language Guidelines**:
          - **RESERVE "Critical/Extreme"** for genuine emergencies only (fires, evacuations, safety incidents)
          - **Below-target production â‰  critical failure** - use measured, professional language
          - **Silo levels**: NOT critical daily parameter - low levels = normal operations
          - **Professional tone**: Context-aware analysis appropriate for UG Mining Department
          
          ## How to Process Production Reports
          
          ### Step 1: Read Templates and Create Processing Plan
          **MANDATORY FIRST ACTIONS**:
          1. Read `Report Templates/Standard Mine Site Report Template.md`
          2. Read `Report Templates/Shafts & Winders Report Template.md`
          3. Create processing plan checklist:
             ```
             ### Production Report Processing Plan
             - [ ] Read both report templates for structure guidance
             - [ ] Read and analyze WhatsApp data source
             - [ ] Identify mine sites present in data
             - [ ] Extract production metrics following template schemas
             - [ ] Create JSON files with complete data validation
             - [ ] Create comprehensive Markdown reports using exact template structure
             - [ ] Validate all data points against source
             - [ ] Confirm file creation and professional formatting
             ```
          
          ### Step 2: Data Analysis and Extraction
          1. **Read source data**: Use appropriate tools to examine `whatsapp_data_for_gemini.txt`
          2. **Identify sites**: Look for Gloria, Nchwaning 2, Nchwaning 3, Shafts & Winders mentions
          3. **Parse comprehensive data**:
             - **Safety**: Status, incidents, details
             - **Production**: ROM, Product, Decline vs targets with variance calculations
             - **Loads**: Truckloads tipped by shift (Day/Afternoon/Night)
             - **Blast**: Faces blasted with breakdown details
             - **Equipment Availability**: TMM percentages (DT, FL, HD, RT, SR)
             - **Equipment Readiness**: Start-of-shift availability (equipment, not staffing)
             - **Breakdowns**: Current issues with specific unit numbers
             - **Operational**: Main fans, plant blockages, fire alarms
             - **Site-Specific**: Silo levels (Gloria), Dam levels (S&W), etc.
          
          ### Step 3: Create Comprehensive Files Following Templates
          
          **Directory Structure**: `daily_production/data/YYYY-MM/DD/`
          
          **For each site detected:**
          
          #### 1. Complete JSON File (`YYYY-MM-DD_[site].json`)
          **Follow template JSON schemas exactly**:
          ```json
          {
            "report_metadata": {
              "date": "YYYY-MM-DD",
              "data_date": "YYYY-MM-DD",
              "site": "Site Name",
              "engineer": "Engineer Name",
              "timestamp": "HH:MM",
              "report_type": "Daily Report"
            },
            "safety": {
              "status": "clear/incident",
              "incidents": 0,
              "details": []
            },
            "production": {
              "rom": {"actual": 0, "target": 0, "variance": 0, "variance_percentage": 0.0, "unit": "tonnes"},
              "decline": {"actual": 0, "target": 0, "variance": 0, "variance_percentage": 0.0, "unit": "tonnes"},
              "product": {"actual": 0, "target": 0, "variance": 0, "variance_percentage": 0.0, "unit": "tonnes"},
              "loads": [
                {"shift": "day", "load_number": 1, "truckloads_tipped": 0, "target": null},
                {"shift": "afternoon", "load_number": 2, "truckloads_tipped": 0, "target": null},
                {"shift": "night", "load_number": 3, "truckloads_tipped": 0, "target": null}
              ],
              "blast": {"faces": 0, "breakdown": null, "note": "details"}
            },
            "equipment_availability": {
              "tmm": {"DT": 0, "FL": 0, "HD": 0, "RT": 0, "SR": 0},
              "specialized": [{"code": "equipment_code", "availability": 0}]
            },
            "shift_readiness": {
              "production_tmm": {
                "DT": {"available": 0, "total": 0},
                "FL": {"available": 0, "total": 0},
                "HD": {"available": 0, "total": 0},
                "RT": {"available": 0, "total": 0},
                "SR": {"available": 0, "total": 0}
              }
            },
            "breakdowns": {
              "current_breakdowns": [{"unit": "unit_id", "issue": "description"}]
            },
            "operational": {
              "main_fans": "status",
              "plant_blockages": 0,
              "fire_alarms": 0
            },
            "source_validation": {
              "safety_status": {"value": "status", "source_line": "X", "source_quote": "exact text", "confidence": "HIGH"},
              "rom_actual": {"value": 0, "source_line": "X", "source_quote": "exact text", "confidence": "HIGH"}
              // Include validation for ALL major data points
            }
          }
          ```
          
          #### 2. Professional Markdown Report (`YYYY-MM-DD â€“ [Site Name] Daily Report.md`)
          **CRITICAL: Follow template structure exactly**:
          
          **Standard Mine Site Reports** (N2, N3, Gloria):
          ```markdown
          ---
          JSONData:: [[YYYY-MM-DD_[site].json]]
          ---
          
          # [Site Name] Daily Report
          **Date**: Month DD, YYYY (Data from Month DD, YYYY)  
          **Engineer**: [[Engineer Name]]  
          **Site**: [Site Name]  
          
          ## [ğŸŸ¢âœ…ğŸŸ¡âš ï¸ğŸ”´ğŸš¨] [STATUS SUMMARY]
          
          ### Safety Status
          [âœ… **CLEAR** - No incidents] OR [ğŸ”´ **INCIDENT** - Details]
          
          ### Production Performance
          [ğŸŸ¢ğŸŸ¡ğŸ”´] **[COMPREHENSIVE PERFORMANCE ANALYSIS]**
          
          | Metric | Actual | Target | Variance | Performance |
          |--------|--------|--------|----------|-------------|
          | **ROM** | X,XXXt | X,XXXt | Â±XXXt | **XX.X% (XX.X% of target)** [ğŸŸ¢âš ï¸ğŸ”´] |
          | **Decline** | X,XXXt | X,XXXt | Â±XXXt | **XX.X% (XX.X% of target)** [ğŸŸ¢âš ï¸ğŸ”´] |
          | **Product** | X,XXXt | X,XXXt | Â±XXXt | **XX.X% (XX.X% of target)** [ğŸŸ¢âš ï¸ğŸ”´] |
          
          **Waste Production**: [Details if available]
          
          #### Load & Haul Fleet Performance (Truckloads Tipped by Shift)
          | Shift | Loads | Target | Performance |
          |-------|-------|--------|-------------|
          | Day | XX | - | **[Performance description]** [ğŸŸ¢âš ï¸ğŸ”´] |
          | Afternoon | XX | - | **[Performance description]** [ğŸŸ¢âš ï¸ğŸ”´] |
          | Night | XX | - | **[Performance description]** [ğŸŸ¢âš ï¸ğŸ”´] |
          | **Total** | **XXX** | **-** | **[Overall assessment]** |
          
          [Additional performance context, staging notes, shift-specific issues]
          
          #### Blast Performance
          - **[Specific blast data]** OR **Nothing Reported**
          
          ### Equipment Status
          
          #### TMM Availability
          | Equipment | Availability | Status |
          |-----------|-------------|---------|
          | **DT** | **XX%** | [ğŸŸ¢âœ…âš ï¸ğŸ”´] [Detailed status description] |
          | **FL** | **XX%** | [ğŸŸ¢âœ…âš ï¸ğŸ”´] [Detailed status description] |
          | **HD** | **XX%** | [ğŸŸ¢âœ…âš ï¸ğŸ”´] [Detailed status description] |
          | **RT** | **XX%** | [ğŸŸ¢âœ…âš ï¸ğŸ”´] [Detailed status description] |
          | **SR** | **XX%** | [ğŸŸ¢âœ…âš ï¸ğŸ”´] [Detailed status description] |
          
          **Poor Performance Analysis:**
          - **[Equipment Type]**: [Specific analysis of issues, breakdowns, maintenance needs]
          - **[Equipment Type]**: [Detailed explanations for sub-optimal performance]
          
          #### Equipment Readiness (Start of Shift)
          - **DT**: X/X available (XX%) [ğŸŸ¢âš ï¸ğŸ”´]
          - **FL**: X/X available (XX%) [ğŸŸ¢âš ï¸ğŸ”´]
          - **HD**: X/X available (XX%) [ğŸŸ¢âš ï¸ğŸ”´]
          - **RT**: X/X available (XX%) [ğŸŸ¢âš ï¸ğŸ”´]
          - **SR**: X/X available (XX%) [ğŸŸ¢âš ï¸ğŸ”´]
          
          ### Current Breakdowns (X Units)
          
          #### [Equipment Type] Equipment Issues
          - **[Unit ID]**: [Detailed issue description]
          - **[Unit ID]**: [Detailed issue description]
          
          #### [Additional Equipment Types]
          [Continue for all equipment categories with issues]
          
          ### Support Equipment Summary
          - **[Equipment Category]**: XX% [ğŸŸ¢âš ï¸ğŸ”´] [Status description]
          - **[Specialized Equipment]**: [Status] [ğŸŸ¢ğŸ”´] [Details]
          
          ### Infrastructure Status
          - **Main Fans**: [Status] [ğŸŸ¢ğŸ”´]
          - **Plant Blockages**: [None/Count with locations] [ğŸŸ¢ğŸ”´]
          - **Fire Alarms**: [None/Active details] [ğŸŸ¢ğŸ”´]
          
          ## Performance Summary
          - **Safety**: [Comprehensive status assessment] [ğŸŸ¢ğŸ”´]
          - **Production**: [Detailed performance analysis] [ğŸŸ¢ğŸ”´âš ï¸]
          - **Equipment**: [Fleet availability and reliability assessment] [ğŸŸ¢ğŸ”´âš ï¸]
          - **Operations**: [Overall operational status] [ğŸŸ¢ğŸ”´âš ï¸]
          
          ## Supplemental Information
          
          ### [Site-Specific Sections]
          [Gloria: Silo Management with surface/underground tables]
          [N2/N3: Additional operational context]
          [Detailed breakdown analysis, trends, operational notes]
          
          ### [Critical Issues Section if applicable]
          1. **[Priority Level] [URGENCY]**: [Detailed issue description and impact]
          2. **[Priority Level] [URGENCY]**: [Action items and recommendations]
          
          ---
          *Report processed: YYYY-MM-DD | Data period: YYYY-MM-DD | Source: WhatsApp HH:MM*
          
          #daily-production #[site-tag] #[status-tags] #[engineer-tag] #year/YYYY
          ```
          
          ### Step 4: Data Validation & Quality Assurance
          
          **CRITICAL DATA INTEGRITY RULES**:
          - **NEVER INVENT DATA**: Extract only actual values from WhatsApp source
          - **Source Traceability**: Every major data point must include source validation
          - **Use null for missing data**: If metric not in source, use `null` not fabricated values
          - **Quote Accuracy**: Include exact quotes from WhatsApp messages
          - **Confidence Scoring**: HIGH/MEDIUM/LOW based on source clarity
          - **Equipment Code Validation**: Verify all codes against reference standards
          - **Professional Language**: Avoid hyperbolic descriptions, use measured analysis
          
          ### Step 5: File Organization and Verification
          
          1. **Directory Structure**: Ensure `daily_production/data/YYYY-MM/DD/` exists
          2. **File Creation**: Generate both JSON and Markdown for each detected site
          3. **Template Compliance**: Verify reports match template structure and formatting
          4. **Professional Quality**: Ensure reports meet Claude-quality standards
          5. **Validation**: Confirm all major data points include source validation
          
          ## Template Usage Guidelines
          
          ### Status Icons & Professional Formatting
          - **ğŸŸ¢âœ…** = Good/Excellent/Operational (use for >90% performance)
          - **ğŸŸ¡âš ï¸** = Warning/Below optimal/Attention needed (70-90% range)
          - **ğŸ”´** = Critical/Problem/Below threshold (<70% performance)
          - **ğŸš¨** = Emergency/Safety incident/Immediate action required
          
          ### Priority Classifications
          - **ğŸ”´ URGENT** = Immediate action required (safety, operational stoppage)
          - **ğŸ”´ HIGH** = High priority action within 24 hours
          - **âš ï¸ MEDIUM** = Medium priority attention within week
          - **ğŸ“Š FOLLOW-UP** = Monitoring/data gathering required
          - **ğŸ“‹ ROUTINE** = Standard operational task
          
          ### Professional Analysis Standards
          - **Performance Analysis**: Include context, trends, operational impact
          - **Equipment Assessment**: Detailed breakdown analysis with root causes
          - **Production Interpretation**: Variance analysis with business context
          - **Operational Context**: Consider shift patterns, planned maintenance, external factors
          
          ## Success Criteria
          
          - [ ] Read and understood both report templates
          - [ ] WhatsApp data comprehensively analyzed
          - [ ] All mine sites identified and processed
          - [ ] JSON files created with complete schema validation
          - [ ] Markdown reports follow template structure exactly
          - [ ] Professional-quality analysis and formatting
          - [ ] All data points validated against source
          - [ ] Equipment codes properly validated
          - [ ] Files properly organized in directory structure
          - [ ] Processing meets Claude-quality standards
          
          **Begin processing now - prioritize template compliance and professional quality.**
    
    - name: Verify File Creation
      id: verify_files
      if: steps.gemini_process.conclusion == 'success'
      run: |
        TARGET_DATE="${{ github.event.client_payload.date || github.event.inputs.date }}"
        YEAR_MONTH=$(echo $TARGET_DATE | cut -d'-' -f1,2)
        DAY=$(echo $TARGET_DATE | cut -d'-' -f3)
        TARGET_DIR="daily_production/data/${YEAR_MONTH}/${DAY}"
        
        echo "ğŸ” Verifying files were created by Gemini production processing in: $TARGET_DIR"
        
        # Count actual files created
        JSON_FILES=$(find "$TARGET_DIR" -name "*.json" 2>/dev/null | wc -l || echo "0")
        MD_FILES=$(find "$TARGET_DIR" -name "*.md" 2>/dev/null | wc -l || echo "0")
        TOTAL_FILES=$((JSON_FILES + MD_FILES))
        
        echo "ğŸ“Š Verification: $JSON_FILES JSON, $MD_FILES Markdown = $TOTAL_FILES total files"
        
        # List created files
        if [ -d "$TARGET_DIR" ]; then
          echo "ğŸ“‚ Files in $TARGET_DIR:"
          ls -la "$TARGET_DIR" || echo "Directory exists but is empty"
        else
          echo "âš ï¸ Target directory does not exist"
        fi
        
        # Validation check
        if [ "$TOTAL_FILES" -gt 0 ]; then
          echo "âœ… Files were created by Gemini ($TOTAL_FILES total)"
        else
          echo "âŒ No files were created by Gemini"
        fi
        
        echo "json_files_count=$JSON_FILES" >> $GITHUB_OUTPUT
        echo "md_files_count=$MD_FILES" >> $GITHUB_OUTPUT
        echo "total_files=$TOTAL_FILES" >> $GITHUB_OUTPUT
        echo "target_dir=$TARGET_DIR" >> $GITHUB_OUTPUT

    - name: Commit and Push Generated Files
      if: steps.verify_files.outputs.total_files > 0
      run: |
        TARGET_DATE="${{ github.event.client_payload.date || github.event.inputs.date }}"
        
        # Configure git user
        git config --local user.name "gemini-ai[bot]"
        git config --local user.email "gemini-ai[bot]@users.noreply.github.com"
        
        # Add generated files
        git add daily_production/data/ || echo "No files to add"
        
        # Check if there are changes to commit
        if git diff --cached --quiet; then
          echo "â„¹ï¸ No changes to commit"
        else
          # Commit the generated files
          git commit -m "feat: Add Gemini-generated daily reports for $TARGET_DATE

          ğŸ“Š Generated Files: ${{ steps.verify_files.outputs.total_files }} total
          ğŸ¤– Generated with Gemini 2.5 Flash via GitHub Actions"
          
          # Push to repository
          git push origin master
          echo "âœ… Files committed and pushed to repository"
        fi