name: WhatsApp Extraction + Gemini Processing Pipeline

on:
  workflow_dispatch:
    inputs:
      processing_date:
        description: 'Date to process (YYYY-MM-DD)'
        required: true
        default: '2025-07-06'
      site_filter:
        description: 'Site filter (optional: gloria, nchwaning2, nchwaning3, shafts_winders)'
        required: false
      test_mode:
        description: 'Test mode (process only one file)'
        type: boolean
        default: false

  issues:
    types: [labeled]

jobs:
  extract-and-process:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'issues' && contains(github.event.label.name, 'gemini-process'))
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests google-generativeai python-dateutil
        
    - name: Set processing parameters
      id: params
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "date=${{ github.event.inputs.processing_date }}" >> $GITHUB_OUTPUT
          echo "site_filter=${{ github.event.inputs.site_filter }}" >> $GITHUB_OUTPUT
          echo "test_mode=${{ github.event.inputs.test_mode }}" >> $GITHUB_OUTPUT
        else
          # Extract date from issue title or use default
          echo "date=2025-07-06" >> $GITHUB_OUTPUT
          echo "site_filter=" >> $GITHUB_OUTPUT
          echo "test_mode=true" >> $GITHUB_OUTPUT
        fi
        
    - name: Extract WhatsApp Data from Codespace
      env:
        GH_TOKEN: ${{ secrets.PAT_WITH_CODESPACE }}
      run: |
        echo "🚀 Extracting WhatsApp data from Codespace..."
        
        # Create extraction script
        cat > extract_whatsapp_data.py << 'EOF'
import subprocess
import json
import os
from datetime import datetime, timedelta

def extract_data_for_date(target_date):
    """Extract WhatsApp production data for specific date"""
    
    codespace_name = "cuddly-guacamole-496vp6p46wg39r"
    db_path = "/workspaces/MarthaVault/whatsapp-mcp/whatsapp-bridge/store/messages.db"
    
    # Construct SQL query for the target date
    start_time = f"{target_date} 00:00:00+00:00"
    end_time = f"{target_date} 23:59:59+00:00"
    
    sql_query = f"""
    SELECT timestamp, chat_jid, sender, substr(content, 1, 2000)
    FROM messages 
    WHERE timestamp BETWEEN '{start_time}' AND '{end_time}'
    AND (content LIKE '%ROM%' OR content LIKE '%Production%' OR content LIKE '%Gloria%' 
         OR content LIKE '%Nchwaning%' OR content LIKE '%S&W%')
    ORDER BY timestamp;
    """
    
    # Execute query via codespace SSH
    cmd = [
        "gh", "codespace", "ssh", "-c", codespace_name, "--",
        "sqlite3", db_path, sql_query
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        # Parse results and create structured data
        lines = result.stdout.strip().split('\n')
        extracted_data = []
        
        for line in lines:
            if line.strip():
                parts = line.split('|')
                if len(parts) >= 4:
                    timestamp, chat_jid, sender, content = parts[0], parts[1], parts[2], parts[3]
                    
                    # Determine site from content
                    site = "Unknown"
                    if "Gloria" in content or "GL" in content:
                        site = "Gloria"
                    elif "Nchwaning 2" in content or "N2" in content:
                        site = "Nchwaning 2"
                    elif "Nchwaning 3" in content or "N3" in content:
                        site = "Nchwaning 3"
                    elif "S&W" in content or "Shafts" in content:
                        site = "Shafts & Winders"
                    
                    extracted_data.append({
                        "timestamp": timestamp,
                        "chat_jid": chat_jid,
                        "sender": sender,
                        "content": content,
                        "site": site,
                        "report_date": target_date
                    })
        
        return extracted_data
        
    except subprocess.CalledProcessError as e:
        print(f"❌ Extraction failed: {e}")
        print(f"Error output: {e.stderr}")
        return []

if __name__ == "__main__":
    import sys
    target_date = sys.argv[1] if len(sys.argv) > 1 else "2025-07-06"
    
    print(f"📊 Extracting data for {target_date}")
    data = extract_data_for_date(target_date)
    
    if data:
        # Create output directory
        os.makedirs("extracted_data", exist_ok=True)
        
        # Save raw extraction
        with open(f"extracted_data/{target_date}_raw.json", "w") as f:
            json.dump(data, f, indent=2)
        
        # Group by site and create structured files
        sites_data = {}
        for item in data:
            site = item["site"]
            if site not in sites_data:
                sites_data[site] = []
            sites_data[site].append(item)
        
        # Create site-specific files
        for site, messages in sites_data.items():
            if site != "Unknown":
                site_filename = site.lower().replace(" ", "_").replace("&", "")
                
                # Combine messages for this site
                combined_content = "\n\n".join([msg["content"] for msg in messages])
                
                # Engineer mapping
                engineer_map = {
                    "gloria": "Sipho Dubazane",
                    "nchwaning_2": "Johan Kotze (acting for Sikilela Nzuza)",
                    "nchwaning_3": "Sello Sease",
                    "shafts_winders": "Xavier Peterson"
                }
                
                structured_data = {
                    "report_metadata": {
                        "site": site,
                        "engineer": engineer_map.get(site_filename, "Unknown"),
                        "report_date": target_date,
                        "data_date": target_date,
                        "extraction_timestamp": datetime.now().isoformat(),
                        "message_count": len(messages)
                    },
                    "raw_content": combined_content,
                    "individual_messages": messages
                }
                
                output_file = f"extracted_data/{target_date}_{site_filename}.json"
                with open(output_file, "w") as f:
                    json.dump(structured_data, f, indent=2, ensure_ascii=False)
                
                print(f"✅ Created: {output_file}")
        
        print(f"📁 Extracted {len(data)} messages for {len(sites_data)} sites")
    else:
        print("❌ No data extracted")
EOF

        python extract_whatsapp_data.py "${{ steps.params.outputs.date }}"
        
    - name: Process with Gemini
      env:
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        echo "🤖 Processing extracted data with Gemini..."
        
        # Copy our integration script
        cp scripts/integrate-gemini-processing.py ./process_with_gemini.py
        
        # Process the extracted data
        if [ "${{ steps.params.outputs.test_mode }}" = "true" ]; then
          python process_with_gemini.py --input-dir extracted_data --output-dir gemini_processed --test-mode
        else
          python process_with_gemini.py --input-dir extracted_data --output-dir gemini_processed --date-filter "${{ steps.params.outputs.date }}"
        fi
        
    - name: Validate Processing Results
      run: |
        echo "🔍 Validating Gemini processing results..."
        
        # Create validation script
        cat > validate_processing.py << 'EOF'
import json
import os
from pathlib import Path

def validate_gemini_output(file_path):
    """Validate processed Gemini output"""
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # Check required top-level fields
        required_fields = [
            'report_metadata', 'safety', 'production', 
            'equipment_availability', 'source_validation'
        ]
        
        missing_fields = [field for field in required_fields if field not in data]
        
        if missing_fields:
            return False, f"Missing required fields: {missing_fields}"
        
        # Check source validation
        source_val = data.get('source_validation', {})
        if not source_val:
            return False, "Missing source validation section"
        
        # Check metadata
        metadata = data.get('report_metadata', {})
        if not metadata.get('site') or not metadata.get('date'):
            return False, "Missing required metadata"
        
        return True, "Valid"
        
    except Exception as e:
        return False, f"Validation error: {e}"

# Validate all processed files
processed_dir = Path("gemini_processed")
if processed_dir.exists():
    results = []
    for json_file in processed_dir.glob("*.json"):
        is_valid, message = validate_gemini_output(json_file)
        results.append({
            'file': str(json_file),
            'valid': is_valid,
            'message': message
        })
        
        status = "✅" if is_valid else "❌"
        print(f"{status} {json_file.name}: {message}")
    
    # Summary
    valid_count = sum(1 for r in results if r['valid'])
    total_count = len(results)
    print(f"\n📊 Validation Summary: {valid_count}/{total_count} files passed")
    
    # Save validation report
    with open('validation_results.json', 'w') as f:
        json.dump(results, f, indent=2)
else:
    print("❌ No processed files found")
EOF

        python validate_processing.py
        
    - name: Upload Processing Artifacts
      uses: actions/upload-artifact@v3
      with:
        name: gemini-processed-reports-${{ steps.params.outputs.date }}
        path: |
          extracted_data/
          gemini_processed/
          validation_results.json
        retention-days: 30
        
    - name: Create Summary Report
      run: |
        echo "📋 Creating processing summary..."
        
        # Count results
        extracted_count=$(find extracted_data -name "*.json" 2>/dev/null | wc -l || echo "0")
        processed_count=$(find gemini_processed -name "*.json" 2>/dev/null | wc -l || echo "0")
        
        # Create summary
        cat > processing_summary.md << EOF
        # Gemini Processing Summary - ${{ steps.params.outputs.date }}

        ## Results Overview
        - **Date Processed**: ${{ steps.params.outputs.date }}
        - **WhatsApp Messages Extracted**: ${extracted_count} files
        - **Gemini Processed Reports**: ${processed_count} files
        - **Processing Time**: $(date)

        ## Processing Pipeline Status
        ✅ **Phase 1**: WhatsApp data extraction from Codespace  
        ✅ **Phase 2**: Gemini-1.5-Pro template processing  
        ✅ **Phase 3**: Validation and artifact creation  

        ## Next Steps
        - Download artifacts to review processed reports
        - Manual validation of production figures recommended
        - Ready for Claude Code review and integration

        ## Files Generated
        $(find gemini_processed -name "*.json" 2>/dev/null | sed 's/^/- /' || echo "No processed files found")

        ---
        *Automated processing completed by GitHub Actions + Gemini integration*
        EOF

        echo "📄 Processing Summary:"
        cat processing_summary.md